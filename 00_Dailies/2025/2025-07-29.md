- 然后你们可以先去学习一下PyTorch里面自定义函数自动微分的方法，https://docs.pytorch.org/docs/stable/notes/extending.html#extending-autograd。我之前写了一个Tensorflow版本的，可以辅助理解，https://statr.me/2022/10/custom-grad-in-tensorflow/
- 说起来你们想不想建一个代码仓库来实现differentiable ReHLine的算法？这个可以和写文章同时进行，也算是一份研究经历，可以写到简历和PS里面去的。可以参照这两个库的结构：https://github.com/google-research/fast-soft-sort/，https://github.com/teddykoker/torchsort。我建议先按我上面说的那个方法简化一下求逆的运算，然后把ReLU这个版本的代码实现一下，写成一个在PyTorch中能自动微分的组件。这样至少有了一个可以对外展示的项目，然后后面再慢慢完善