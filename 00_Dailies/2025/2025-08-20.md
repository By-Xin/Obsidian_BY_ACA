这里的几个上面提到的逻辑我改过来了, 感觉因为就是具体公式的计算问题, 比较直接. 一个是把多余的 xi 的判断逻辑去掉了, 第二个明确了一下 pi 的求解.  但是又过一遍代码的时候发现之前讨论过的一个问题还是有点反应不过来: 

在标准的 ReHLine 中, 以标准 SVM 为例的话, 我们可以根据 svm 的目标函数 $\min_\beta \frac{C}{n} \sum_{i=1}^n \max (0, 1 - y_i\beta^\top x_i) + \frac{1}{2} ||\beta||_2^2$ 对应转换, 然后这里的 $y_i$ 根据 ReHLine 形式就被吸收到 $U$ 里了 (如果我没理解错应该相当于 $L=1$, $U_{1,i}  = - \frac{C}{n} y_i$ (? 

但是在我们的 autoloss 的形式中, 完全有可能 $L>1$, 并且我们这里的 $U_{l,i}$ 也是一个可学习的参数. 那对应的最直接的问题就是这里我们即使初始化的时候按照 $- \frac{C}{n} y_i$ 进行, 但是我们没法在学习的时候“注入” $y$ 的信息, 或者这个 supervised 的 label 出现在了被训练的参数位置? 这个可能是目前比较迷惑的地方. 难道说我们所有关于 label 的信息都是在外层循环 $y_{\text{val}}$ 中学到的吗, 这个感觉应该是我迷糊了. 

说到底可能还是在整个 autoloss 的整体框架上, 不知道该怎么安排合理一点. 但是另一方面非常神奇的是, 即使如此, 我们最后学到的系数还是比较接近的, 分类准确率也比较高, 但是系数又同时是 10 倍...捂脸. 我后面还是打算再彻底 review 一下我这个代码, 然后一个板块一个板块写成 ipynb 整理一下思路. 也再过一遍 ReHLine. 不过可能确实得先弄清上面卡住那里才好

![[Pasted image 20250820193402.png]]