# cs.CL - Computation and Language (NLP & LLM)

> è‡ªç„¶è¯­è¨€å¤„ç†ä¸å¤§è¯­è¨€æ¨¡å‹ | arXiv: [cs.CL](https://arxiv.org/list/cs.CL/recent)

---

## ğŸ“ ç›®å½•ç»“æ„

```
cs.CL_NLP_LLM/
â”œâ”€â”€ Embeddings/                    # è¯åµŒå…¥
â”‚   â”œâ”€â”€ README.md                  # è¯å‘é‡æ¦‚è¿°
â”‚   â”œâ”€â”€ Word2Vec.md                # Word2Vec (ç†è®º+å®ç°)
â”‚   â”œâ”€â”€ Negative_Sampling.md       # è´Ÿé‡‡æ ·ä¼˜åŒ–
â”‚   â””â”€â”€ GloVe.md                   # å…¨å±€å‘é‡
â”‚
â”œâ”€â”€ Pretrained_Models/             # é¢„è®­ç»ƒæ¨¡å‹
â”‚   â””â”€â”€ BERT.md                    # BERT æ¨¡å‹
â”‚
â”œâ”€â”€ LLM_Training/                  # LLM è®­ç»ƒ
â”‚   â”œâ”€â”€ Pretrain_and_Alignment_for_LLMs.md
â”‚   â””â”€â”€ Post-Training_and_Forgetting.md
â”‚
â””â”€â”€ LLM_Inference/                 # LLM æ¨ç†ä¸åˆ†æ
    â”œâ”€â”€ Deep_Reasoning_for_LLMs.md
    â”œâ”€â”€ Uncertainty_in_LLMs.md
    â””â”€â”€ Context_Engineering.md
```

---

## ğŸ“ ç¬”è®°ç´¢å¼•

### Embeddings/ - è¯åµŒå…¥

| æ–‡ä»¶ | ä¸»é¢˜ | å…³é”®æ¦‚å¿µ |
|------|------|----------|
| `README.md` | è¯å‘é‡æ¦‚è¿° | æ–¹æ³•åˆ†ç±», One-hot vs Dense |
| `Word2Vec.md` | Word2Vec æ¨¡å‹ | Skip-gram, CBOW, ç¥ç»ç½‘ç»œè§†è§’, PyTorch å®ç° |
| `Negative_Sampling.md` | è´Ÿé‡‡æ ·æŠ€å·§ | é‡‡æ ·åˆ†å¸ƒ, äºŒåˆ†ç±»è¿‘ä¼¼ |
| `GloVe.md` | GloVe æ¨¡å‹ | Co-occurrence Matrix, PMI, å…¨å±€å‘é‡ |

### Pretrained_Models/ - é¢„è®­ç»ƒæ¨¡å‹

| æ–‡ä»¶ | ä¸»é¢˜ | å…³é”®æ¦‚å¿µ |
|------|------|----------|
| `NGram_LanguageModels.md` | è¯­è¨€æ¨¡å‹åŸºç¡€ | N-gram, Markov å‡è®¾ |
| `BERT.md` | BERT æ¨¡å‹ | Self-Supervised Learning, Pre-training, Fine-tuning |

### LLM_Training/ - LLM è®­ç»ƒ

| æ–‡ä»¶ | ä¸»é¢˜ | å…³é”®æ¦‚å¿µ |
|------|------|----------|
| `Pretrain_and_Alignment_for_LLMs.md` | é¢„è®­ç»ƒä¸å¯¹é½ | SFT, RLHF, Knowledge Distillation |
| `Post-Training_and_Forgetting.md` | åè®­ç»ƒä¸é—å¿˜ | Catastrophic Forgetting, Continual Learning |

### LLM_Inference/ - LLM æ¨ç†ä¸åˆ†æ

| æ–‡ä»¶ | ä¸»é¢˜ | å…³é”®æ¦‚å¿µ |
|------|------|----------|
| `Deep_Reasoning_for_LLMs.md` â­ | LLM æ·±åº¦æ¨ç† | Chain of Thought, Test-Time Compute, RL |
| `Uncertainty_in_LLMs.md` â­ | ä¸ç¡®å®šæ€§é‡åŒ– | Semantic Entropy, Confidence Estimation |
| `Context_Engineering.md` | ä¸Šä¸‹æ–‡å·¥ç¨‹ | Prompt Engineering, In-Context Learning |

---

## ğŸ“š ç›¸å…³ä¸“é¢˜ (20_StudyNotes/Topics/)

| æ–‡ä»¶ | ä¸»é¢˜ |
|------|------|
| `LLM_Sampling_Methods.md` | é‡‡æ ·æ–¹æ³•ç»¼è¿° (Top-k, Top-p, Temperature, Mirostat) |
| `Latent_Space_LLM_Survey_2024.md` | æ½œç©ºé—´ LLM å‰æ²¿æŠ€æœ¯ç»¼è¿° |

## ğŸ“„ ç›¸å…³è®ºæ–‡ (20_StudyNotes/Papers/)

| æ–‡ä»¶ | ä¸»é¢˜ |
|------|------|
| `Mitigating_LLM_Hallucinations_via_Conformal_Abstention.md` | å¹»è§‰ç¼“è§£ (Conformal Prediction) |

---

## ğŸ—ºï¸ çŸ¥è¯†å›¾è°±

```
è¯åµŒå…¥ (Static Embeddings)
  â”œâ”€ Embeddings/README (æ¦‚è¿°)
  â”‚     â”œâ”€â†’ Word2Vec (Skip-gram/CBOW + å®ç°)
  â”‚     â”‚     â””â”€â†’ Negative_Sampling (ä¼˜åŒ–æŠ€å·§)
  â”‚     â””â”€â†’ GloVe (å…¨å±€å‘é‡, PMI)
  â”‚
é¢„è®­ç»ƒæ¨¡å‹
  â”œâ”€ BERT (Contextual Embeddings)
  â”‚
LLM è®­ç»ƒ
  â”œâ”€ Pretrain_and_Alignment (SFT, RLHF)
  â”‚     â””â”€â†’ Post-Training_and_Forgetting (é—å¿˜é—®é¢˜)
  â”‚
LLM æ¨ç†ä¸åˆ†æ
  â”œâ”€ Deep_Reasoning_for_LLMs (æ¨ç†èƒ½åŠ›)
  â”‚     â””â”€â†’ [Topics] Latent_Space_LLM_Survey
  â”‚
  â”œâ”€ Uncertainty_in_LLMs (ç½®ä¿¡åº¦)
  â”‚     â””â”€â†’ [Papers] Conformal_Abstention
  â”‚
  â””â”€ Context_Engineering (æç¤ºå·¥ç¨‹)
```

---

## ğŸ”— è·¨ç›®å½•å…³è”

- **ç†è®ºåŸºç¡€**: `cs.IT_InfoTheory/` (ç†µã€KLæ•£åº¦ã€äº’ä¿¡æ¯)
- **æ·±åº¦å­¦ä¹ **: `ml.DL_DeepLearning/` (Transformer, Attention)
- **æœºå™¨å­¦ä¹ **: `ml.AL_Algorithms/` (Transfer Learning, LifeLong Learning)
- **ç»Ÿè®¡æ¨æ–­**: `stat.IN_Inference/` (Conformal Prediction)
