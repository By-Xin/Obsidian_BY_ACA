#StatisticalLearning 

- Similar to [[AdaBoost]], ***Gradient Boosting*** is an ensemble learning also by adding models to the ensemble, making the new model correct the errors of its predecessor. While AdaBoost increases the weights of the misclassified instances, Gradient Boosting fits the new model to the residual errors made by the previous model.
- Some popular implementations of Gradient Boosting are:
  - GBDT (Gradient Boosting Decision Trees)
  - [[XGBoost]]