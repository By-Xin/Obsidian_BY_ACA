作者：CodeCrafter  
链接：https://www.zhihu.com/question/1976808337942278953/answer/1979658909364024545  
来源：知乎  
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。  
  

这就是典型的**想得太多，抄得太少**。

看到这个问题我其实挺感慨的，因为我也带过不少实习生和校招的新人，这种**比baseline差两倍**的情况，几乎每个季度我都能在组里的周报上看到一两次。这不是你脑子笨，也不是你不适合搞算法，纯粹是你的**工程落地姿势不对**。

很多研究生，特别是研一研二刚开始搞科研的时候，都有个巨大的误区，觉得搞算法就是要从头到尾自己敲一个个字符，仿佛只有import torch是允许复用的，其他的模型结构、数据处理都要自己手搓才叫**科研**。

大错特错。

你要搞清楚你现在的核心矛盾是什么。面临开题，实验没结果。这时候你需要的不是展示你的Python代码有多优雅，能不能写装饰器，能不能写元类，你需要的是**一张能看的表，一条能收敛的曲线**。

这里我给你拆解一下，为什么你的精度会崩成这个鬼样，以及怎么用最快的方式把实验抢救回来。咱们不讲虚的，直接上硬菜。

**第一阶段：停止你的“原创”代码，全盘拿来主义**

既然你的精度比baseline差两倍，我敢打包票，你的baseline实现本身就是错的。

你现在的首要任务不是去改进[SOTA](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=SOTA&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJTT1RBIiwiemhpZGFfc291cmNlIjoiZW50aXR5IiwiY29udGVudF9pZCI6NzU5NDMzNjQxLCJjb250ZW50X3R5cGUiOiJBbnN3ZXIiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.SD9Cn08KUXC6GTD5bH4GGDe-gB_A23BxQho2GGDedGs&zhida_source=entity)，而是**复现**。很多人看不起复现，觉得这是体力活。实际上，能在自己的服务器环境、自己的数据集上完美复现一篇顶会论文的开源代码，这本身就是一种极强的能力。

你要做的第一件事，去[GitHub](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=GitHub&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJHaXRIdWIiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjo3NTk0MzM2NDEsImNvbnRlbnRfdHlwZSI6IkFuc3dlciIsIm1hdGNoX29yZGVyIjoxLCJ6ZF90b2tlbiI6bnVsbH0.JdNkDWCfW-mLgECkqQU4nBrkZGQVSqtUAGgePAKtNAM&zhida_source=entity)上找和你方向最接近的、Star数最高的、最近还在维护的开源项目。注意，是**成套的框架**，不是那种研究生随手传上去的单个py文件。

比如做检测的去搞[MMDetection](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=MMDetection&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJNTURldGVjdGlvbiIsInpoaWRhX3NvdXJjZSI6ImVudGl0eSIsImNvbnRlbnRfaWQiOjc1OTQzMzY0MSwiY29udGVudF90eXBlIjoiQW5zd2VyIiwibWF0Y2hfb3JkZXIiOjEsInpkX3Rva2VuIjpudWxsfQ.gMCHDUYy8dFHlbMEcKr9KWCh8_tiRYqSOiBeWldj43g&zhida_source=entity)，做分割的看[MMSegmentation](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=MMSegmentation&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJNTVNlZ21lbnRhdGlvbiIsInpoaWRhX3NvdXJjZSI6ImVudGl0eSIsImNvbnRlbnRfaWQiOjc1OTQzMzY0MSwiY29udGVudF90eXBlIjoiQW5zd2VyIiwibWF0Y2hfb3JkZXIiOjEsInpkX3Rva2VuIjpudWxsfQ.CCTJ34UMFS3nRnqjI9Q5OF3JCIqPU2kiLnBwt2JJtbk&zhida_source=entity)或者[Detectron2](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=Detectron2&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJEZXRlY3Ryb24yIiwiemhpZGFfc291cmNlIjoiZW50aXR5IiwiY29udGVudF9pZCI6NzU5NDMzNjQxLCJjb250ZW50X3R5cGUiOiJBbnN3ZXIiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.yREDBCuO__vxJShJpvIuI7r6ARfIUSVJ45WeHYnmJDI&zhida_source=entity)，做NLP的抱紧[HuggingFace Transformers](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=HuggingFace+Transformers&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJIdWdnaW5nRmFjZSBUcmFuc2Zvcm1lcnMiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjo3NTk0MzM2NDEsImNvbnRlbnRfdHlwZSI6IkFuc3dlciIsIm1hdGNoX29yZGVyIjoxLCJ6ZF90b2tlbiI6bnVsbH0.0ibwGT3wJEzUFd2m9XkiJth1xu_cblZsZaFJC0zzxyw&zhida_source=entity)的大腿，做底层视觉的去翻[BasicSR](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=BasicSR&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJCYXNpY1NSIiwiemhpZGFfc291cmNlIjoiZW50aXR5IiwiY29udGVudF9pZCI6NzU5NDMzNjQxLCJjb250ZW50X3R5cGUiOiJBbnN3ZXIiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.8DBD9THDcm9zoYXAhYN9ueuCg96p7HWXWGIsFGo61V4&zhida_source=entity)。

为什么要用这些框架？因为这些框架里的Data Loader、Augmentation、Backbone初始化、Learning Rate Scheduler都是经过成千上万次验证的。你觉得自己写的ResNet和人家写的ResNet是一样的，我告诉你，绝大多数时候都不一样。

里面的BatchNorm动量设置、卷积层的初始化分布、甚至是Padding的处理方式，哪怕差一点点，在深层网络里经过几十层传导，最后的梯度就能差出十万八千里。

**这就叫工业级基建。**

一定要学会**魔改**而不是**重写**。你现在的代码能力弱，那就不要去写核心逻辑。把人家的Config文件拿过来，改一下数据集路径，改一下类别数，先跑通再说。

如果这一步你做到了，你会发现，你的精度瞬间就能回到一个正常的水平，至少不会比baseline差两倍，顶多差个几个点。

![](https://picx.zhimg.com/80/v2-431153179f5d3a3dec15a7bff373e577_1440w.webp?source=2c26e567)

**第二阶段：为什么精度还是差？数据Pipeline的坑**

好，假设你现在用了开源框架，或者你仔细检查了代码，发现模型结构没问题，但精度还是起不来。

这时候**千万不要去调参**。

千万不要去调什么学习率、Batch Size、Weight Decay。在你的模型甚至没法达到及格线的时候，调这些参数就是在这个垃圾山上雕花，毫无意义。

根据我的经验，**90%的算法问题都出在数据上**。

我之前带过一个实习生，做图像分类，死活训不出来，Loss就是不降。我看了一眼代码，模型写的没问题，[ResNet50](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=ResNet50&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJSZXNOZXQ1MCIsInpoaWRhX3NvdXJjZSI6ImVudGl0eSIsImNvbnRlbnRfaWQiOjc1OTQzMzY0MSwiY29udGVudF90eXBlIjoiQW5zd2VyIiwibWF0Y2hfb3JkZXIiOjEsInpkX3Rva2VuIjpudWxsfQ.mkLpqRDP-1RaFE10xdV8DLJDf49Psgaak12tHHTWOmk&zhida_source=entity)标准版。后来我让他把Data Loader吐出来的Tensor转回图片存下来看看。

结果发现什么？他没做归一化，或者说归一化做反了。像素值本来是0-255，他直接送进去训了，而预训练模型的权重是基于0-1甚至标准正态分布初始化的。这就导致第一层的激活值直接爆炸，梯度全是NaN或者极小，模型根本学不动。

还有一次，搞NLP的同学，Tokenizer用错了，或者Padding Mask没加对，导致模型把填充的0也当成有效信息去学了，那精度能好吗？

所以，**可视化你的数据**。

这是我给所有算法工程师的第一条建议。不要相信你的代码逻辑，要相信你的眼睛。在输入模型前的最后一刻，把数据拦截下来，打印出它的Shape，打印出它的统计值（均值、方差、最大最小），如果是图像就画出来，如果是文本就Decode回去看看通不通顺。

这一步能帮你解决掉剩下一半的“精度差”问题。

这里推荐一个极其重要但经常被忽视的习惯：**过拟合一个小数据集**。

拿你训练集里的10个样本，或者是1个Batch的数据，单独拿出来，死命训。去掉所有的数据增强，Dropout设为0，学习率设个常规值。

这时候，你的模型Loss应该迅速降到0，Training Accuracy应该达到100%。

如果做不到这一点，说明你的模型代码逻辑有硬伤，或者你的数据有致命缺陷。如果能做到，说明你的模型至少有了“记忆”能力，这时候再去上全量数据。

![](https://pic1.zhimg.com/80/v2-04e9883e79d097e8bf37b237ac8d7b62_1440w.webp?source=2c26e567)

**第三阶段：如何面对SOTA的碾压**

你说你跟SOTA比差到不行。

**废话，SOTA是那么好比的吗？**

现在的Paper，尤其是顶会Paper，里面的Tricks多到你想象不到。很多SOTA不仅仅是模型结构的创新，更是如果你不仔细看附录根本发现不了的**工程Trick堆叠**。

比如Ema（指数移动平均），比如各种花哨的Mixup、CutMix数据增强，比如多尺度训练，比如Test Time Augmentation (TTA)。有些论文为了刷点，甚至会在测试集上做一些不可描述的“优化”。

你一个裸模型，跑去跟人家全副武装的SOTA比，肯定是被按在地上摩擦。

所以，开题或者写小论文，**不要盯着SOTA打**。

你的Baseline选得太高了。你选一个两三年前的经典模型作为Baseline，比如ResNet、[BERT](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=BERT&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJCRVJUIiwiemhpZGFfc291cmNlIjoiZW50aXR5IiwiY29udGVudF9pZCI6NzU5NDMzNjQxLCJjb250ZW50X3R5cGUiOiJBbnN3ZXIiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.xrAUfolJnXaiQ13hoEqHcMqbtAlu1qQjxbJ21LEIcRo&zhida_source=entity)、[UNet](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=UNet&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJVTmV0IiwiemhpZGFfc291cmNlIjoiZW50aXR5IiwiY29udGVudF9pZCI6NzU5NDMzNjQxLCJjb250ZW50X3R5cGUiOiJBbnN3ZXIiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.87TbIEZi6zVyV5wZ5LVg7y8c8GdZRvztP-fEefeRmlI&zhida_source=entity)这种老实稳重的，在这个基础上，加上你的改进点。

只要你的改进点，比这个**原始Baseline**高，那就是有效改进。至于比不比得过最新的SOTA，那是你将来发顶会要考虑的事，发个小论文或者应付开题，只要逻辑自洽，提升明显，足够了。

你需要学会**降维打击**。不要去卷结构，去卷**场景**，去卷**分析**。

代码能力差，咱就不写那种几千行的复杂Transformer架构。咱可以在Loss Function上动动脑子，加个正则项？可以在数据采样策略上搞搞新意，比如给难样本加权？可以在特征融合的方式上做个简单的Concat或者Attention？

这些改动，代码量通常不超过20行，但是往往能带来肉眼可见的收益，而且写进论文里显得特别有道理，数学公式也好推导。

**第四阶段：工具与资源，武装到牙齿**

这里我要穿插一些我觉得能救命的资源。既然代码能力弱，就得靠工具来补。

**1. [Papers with Code](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=Papers+with+Code&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJQYXBlcnMgd2l0aCBDb2RlIiwiemhpZGFfc291cmNlIjoiZW50aXR5IiwiY29udGVudF9pZCI6NzU5NDMzNjQxLCJjb250ZW50X3R5cGUiOiJBbnN3ZXIiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.sq1eirlVNpNCgZlpRFpbdfz_qaIuqgn5vh60RJ7d3ko&zhida_source=entity)**  
这个网站大家应该都知道，但很多人用错了。不要光看那个排行榜。你要点进具体的模型页面，看下面的**Implementations**。

找那个**Official**的，如果没有，找带星最多的。下载下来之后，不要直接跑，先看它的`requirements.txt`，环境配好。

更重要的是，看它的**Issues**区。一个开源项目的Issues区是金矿。你遇到的“精度低”、“不收敛”、“显存爆炸”的问题，大概率别人都遇到过。善用搜索，很多时候解决方案就在某个Issue的回复里藏着。

**2. Weights & Biases (WandB)**  
我求求你们了，别再对着控制台那个黑框框看Log了。去装一个wandb。这东西能全自动地帮你记录Loss曲线、学习率变化、梯度的分布，甚至能直接可视化你在验证集上的预测结果。

很多时候你觉得模型没训练好，去WandB上一看，发现验证集Loss其实早就降下去了，只是你的Evaluation代码写错了。或者你会发现梯度一直是在震荡，根本没下降，这时候你就知道该去调学习率或者查数据了。

这种工具能极大地提升你的**Debug效率**。算法工程师的时间是最值钱的，不要浪费在盯着屏幕发呆上。

**3. [Andrej Karpathy](https://zhida.zhihu.com/search?content_id=759433641&content_type=Answer&match_order=1&q=Andrej+Karpathy&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjU3ODczMTQsInEiOiJBbmRyZWogS2FycGF0aHkiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjo3NTk0MzM2NDEsImNvbnRlbnRfdHlwZSI6IkFuc3dlciIsIm1hdGNoX29yZGVyIjoxLCJ6ZF90b2tlbiI6bnVsbH0.YfDaq-wDJiss-vyLT37LQyf00sffpf5aXmQxVimkJCU&zhida_source=entity)的博客和推特**  
如果你英文还行，去翻翻Karpathy（OpenAI前大神，Tesla前AI总监）的博客。他有一篇神文叫 "A Recipe for Training Neural Networks"。

这篇文章详细到了极致，教你怎么从0开始把一个网络训收敛。我上面说的“过拟合小数据集”、“从简单的Baseline开始”，很多核心思想都来自这篇文章。这文章常读常新，我工作这么多年了，每次遇到诡异的问题，回去看一眼，都能找到灵感。

英语在IT行业不是语言，是工具。最新的论文是英文的，最好的文档是英文的，Github上最活跃的讨论是英文的。你英语不好，就像是练武功少了一只手。

这本小册子，在我当时的进阶之路上也是一个很重要的支点。

[程序员必知必会英语手册.pdf](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/nwTxZdRMdf93I9TTHWKE4g)

它打破了一个误区，读懂技术文档其实不需要托福雅思那种大而全的英语能力。它非常精准地把那些高频出现的术语、固定的句式结构提炼了出来，提供了一个极具针对性的切入点，帮我快速跨过了看懂 70%-80%的门槛。

![](https://pic1.zhimg.com/80/v2-b404abaa5cc35d9877d039e17e74cb0d_1440w.webp?source=2c26e567)

**4. ChatGPT / Claude**  
既然你代码能力差，现在的AI辅助编程工具简直是你的救星。

但是，**不要直接问它“帮我写个ResNet”**。你要把你的报错信息，或者你写得乱七八糟的那一段逻辑，扔给它，让它帮你**Refactor**（重构），让它帮你**加注释**，让它帮你**写Unit Test**。

你可以把你的Data Loader代码贴给它，问它：“这段代码在处理ImageNet格式数据时，有什么潜在的Bug吗？”它真的能给你指出来。

**第五阶段：开题与小论文的生存指南**

最后，说回到你马上要面临的开题。

导师和评审老师其实心里都清楚，研一研二的学生很难做出惊天动地的SOTA。他们看重的是什么？是**工作量**和**科学性**。

如果你的精度实在提不上去，怎么办？

**做消融实验（Ablation Study）。**

把你模型里的模块，拆拆减减。加了这个模块提升了多少，减了那个模块下降了多少。把这些实验做得详详细细的。

哪怕你的最终精度没有超过SOTA，但是你通过详实的实验证明了：在**特定场景下**，或者在**特定数据集上**，你的方法是有优势的。或者你深入分析了为什么现在的SOTA在这个特定问题上不管用。

这也是一篇好论文。**Negative Result也是Result**，只要分析得透彻。

比如，大家都用Transformer，你证明了在这个小数据集上，Transformer就是不如三层卷积好使，并且给出了理论解释（比如归纳偏置的问题）。这也是非常棒的Insight。

代码能力差，不是绝症。

算法工程师的核心竞争力，从来不是代码写得有多快，而是**对数据的敏感度**和**解决问题的思路**。

我现在面试，遇到那种LeetCode刷得飞起，但是给他一个真实数据CSV文件他不知道怎么清洗、不知道怎么看分布的人，我照样不要。相反，如果你能告诉我，你遇到Loss不降的时候，是怎么一步步排查Data Loader、怎么检查梯度、怎么做Error Analysis的，我反而觉得你是个可造之材。

别慌。先去把Baseline的代码跑通，把数据可视化做出来，把WandB用起来。

实验做不出来是常态，SOTA也就是那么回事。等你把这些坑都踩一遍，你会发现，所谓的“大神”，无非就是比你多看了几次报错Log，多调了几次学习率，多看烂了几篇Paper的代码而已。