%----------------------------------------------------------------------------------------
%    PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[aspectratio=169,xcolor=dvipsnames]{beamer}
\usetheme{SimplePlus}

\usepackage{hyperref}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables

%----------------------------------------------------------------------------------------
%    TITLE PAGE
%----------------------------------------------------------------------------------------

\title{TOSICA, GET, Baseline and Cell-eval}

\author{XIN Baiying}
%----------------------------------------------------------------------------------------
%    PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

\begin{document}

\begin{frame}
    % Print the title page as the first slide
    \titlepage
\end{frame}

\begin{frame}{Overview}
    % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
    \tableofcontents
\end{frame}

%------------------------------------------------
% \section{A Foundation Model of Transcription Across Human Cell Types}
\section{Transformer for one stop interpretable cell type annotation (TOSICA)}
%------------------------------------------------

\begin{frame}
\begin{center}
    \Large \textbf{Transformer for one stop interpretable cell type annotation (TOSICA)}
    
    \vspace{1cm}
    
    \normalsize Chen J, Xu H, Tao W, Chen Z, Zhao Y, Han JJ
    
    \vspace{0.5cm}
    
    \normalsize \textit{Nature Communications}, 2023
    
    \vspace{0.3cm}
    
    \small DOI: 10.1038/s41467-023-35923-4
    
    \vspace{0.5cm}
    
    \footnotesize \href{https://www.nature.com/articles/s41467-023-35923-4.pdf}{https://www.nature.com/articles/s41467-023-35923-4.pdf}
\end{center}
\end{frame}




\begin{frame}{Introduction}

The general goal of TOSICA is \textbf{cell type annotation}.

\begin{itemize}
    \item Its input is a single-cell transcriptome expression profile $\mathbf{e} \in \mathbb{R}^n$, where $n$ is the number of genes ($10000+$).
    \item The output is the predicted cell type $\hat{y} \in \{1, ..., C\}$, where $C$ is the number of cell types ($10\sim 50$).
\end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}{TOSICA Architecture}

TOSICA is based on the Transformer architecture and introduces a mask mechanism with biological prior knowledge. Its design consists of three layers:

\begin{itemize}
    \item \textbf{Cell Embedding:} Maps each cell's expression to a low-dimensional representation space, where each dimension corresponds to a biological pathway/regulatory module.
    \item \textbf{Multi-Head Self-Attention:} Uses attention mechanism to compute attention between the cell embedding and classification CLS token to obtain contextual information for cell types.
    \item \textbf{Cell-Type Classifier:} Maps attention information to cell type space through fully connected layers to achieve final classification prediction.
\end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}[allowframebreaks]{Cell Embedding}
For each cell $\mathbf{e}\in \mathbb{R}^n$, where $n$ is the number of genes, we aim to map it to a low-dimensional representation through a fully connected network:
$$\mathbf{t} = \mathbf{W'}\mathbf{e}\in\mathbb{R}^k$$ 
where $\mathbf{W'}\in \mathbb{R}^{k\times n}$ is a learnable weight matrix. In this $k$-dimensional space, each dimension serves as a token, and each token represents a biological pathway/regulatory module.

$\quad$

Since we want $\mathbf{t}$ to have each dimension represent a specific biologically meaningful pathway in a structured manner, the weight matrix $\mathbf{W'}$ needs to be specially designed with prior knowledge.

To integrate biological prior knowledge into the cell embedding process:
\begin{itemize}
    
    \item First introduce a general weight matrix $\mathbf{W}\in \mathbb{R}^{k\times n}$
    \item Based on prior knowledge, we construct a mask matrix $\mathbf{M}\in \{0, 1\}^{k\times n}$
    \begin{itemize}
        \item Where $M_{ij} = 1$ if and only if the $i$-th pathway contains the $j$-th gene. This knowledge is obtained from external databases (specifically, gene set datasets from the Gene Set Enrichment Analysis database)
        \item Therefore, only genes within the same pathway are mapped to the same token. This avoids the non-interpretability of mixed components, though it also limits the model's expressive power.
    \end{itemize}
    \item Then perform Hadamard product (element-wise product) on these two matrices to obtain the final weight matrix:
    $$\begin{aligned} \mathbf{W'} &= \mathbf{M} \odot \mathbf{W} \\
    \mathbf{t} &= \mathbf{W'}\mathbf{e}\in\mathbb{R}^k \\
    \end{aligned}$$
\end{itemize}

To enhance performance, the above operation is performed in parallel $m$ times ($m=48$), concatenated to obtain:
    $$\mathbf{T}:=\begin{bmatrix} \mathbf{t}_1 & \mathbf{t}_2 & \cdots & \mathbf{t}_m \end{bmatrix} \in \mathbb{R}^{k\times m}.$$

\end{frame}

%------------------------------------------------

\begin{frame}[allowframebreaks]{Multi-Head Self-Attention}

First, we introduce a learnable dummy token $\mathbf{cls} \in \mathbb{R}^m$ and concatenate it as the first row of $\mathbf{T}$ to obtain the new input:
$$ \mathbf{I} := \begin{bmatrix} \mathbf{cls}^\top \\ \mathbf{T} \end{bmatrix} = \begin{bmatrix} c_1 & c_2 & \cdots & c_m \\ \mathbf{t}_1 & \mathbf{t}_2 & \cdots & \mathbf{t}_m \end{bmatrix} \in \mathbb{R}^{(1+k)\times m}.$$

\begin{itemize}
    \item This is a common practice in Transformer and other NLP models. CLS is an additional, learnable virtual token. Since attention operations output a representation with the same shape as the input, we can use the output of this CLS as the global information extraction result for the current input, similar to a statistic of the current input, serving as the token for subsequent classification.
\end{itemize}

We feed this input $\mathbf{I}$ into the attention mechanism. We first discuss the case of a single head. Overall, we have $\mathbf{O} = \text{Attention}(\mathbf{I}) \in \mathbb{R}^{(1+k)\times m}.$ The specific computational details are as follows:

\begin{itemize}
    \item Through three linear transformations $\mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v \in \mathbb{R}^{(1+k)\times (1+k)}$, we map the input to Query, Key, and Value spaces:
    $$\begin{aligned}
    \mathbf{Q} &= \mathbf{W}_q \mathbf{I} \in \mathbb{R}^{(1+k)\times m}, \\
    \mathbf{K} &= \mathbf{W}_k \mathbf{I} \in \mathbb{R}^{(1+k)\times m}, \\
    \mathbf{V} &= \mathbf{W}_v \mathbf{I} \in \mathbb{R}^{(1+k)\times m}.
    \end{aligned}$$
    
    \item We compute attention scores to obtain contextual information:
    $$\mathbf{A} = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top }{\sqrt{d_k}}\right) \in \mathbb{R}^{(1+k)\times (1+k)}.$$
    where $d_k = m$ is the dimension of the Key vectors. This attention score matrix $\mathbf{A}$ represents the similarity between different tokens.
    
    \item We obtain contextual representations for each token through weighted summation:
    $$\mathbf{O} = \mathbf{A} \mathbf{V} \in \mathbb{R}^{(1+k)\times m}.$$
\end{itemize}

The multi-head case is similar to the above, equivalent to performing $H$ computations in parallel.

\begin{itemize}
    \item For the $h$-th head, we independently introduce learnable weight matrices $\mathbf{W}_q^h, \mathbf{W}_k^h, \mathbf{W}_v^h \in \mathbb{R}^{(1+k)\times (1+k)}$. We finally obtain the output $\mathbf{O}^h = \text{Attention}^h(\mathbf{I}) \in \mathbb{R}^{(1+k)\times m}.$
    
    \item We concatenate the $H$ independent outputs and apply an additional linear mapping to reshape back to the original input shape: $\mathbf{\widetilde{O}} = \mathbf{W}_o \begin{bmatrix} \mathbf{O}^1 & \mathbf{O}^2 & \cdots & \mathbf{O}^H \end{bmatrix} \in \mathbb{R}^{(1+k)\times m}.$
\end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}{Cell-Type Classifier}

Finally, since the attention mechanism's input and output have the same shape, we extract the first row of $\mathbf{\widetilde{O}}$ (denoted as $\widetilde{\mathbf{cls}}$), which is the output of the input CLS token after the attention mechanism, as the global information extraction result for the current input. 

We pass this result through a fully connected neural network to complete classification:
$$\mathbf{p} = \text{softmax}(\mathbf{W}_c \widetilde{\mathbf{cls}})\in \mathbb{R}^C$$
where $\mathbf{W}_c \in \mathbb{R}^{C \times m}$ is the learnable classification weight matrix, and $C$ is the number of cell types.

\end{frame}

%------------------------------------------------

\begin{frame}[allowframebreaks]{Model Architecture}
    The main structure is as follows:
    \begin{figure}
    \includegraphics[width=0.35\linewidth]{fig/Main Structure.png}
    \end{figure}
\break
    The detailed implementation is as follows:
    \begin{figure}
        \includegraphics[width=0.8\linewidth]{fig/Submodule Implementation Structure.png}
    \end{figure}
\end{frame}

%------------------------------------------------

\begin{frame}{Training Details}
\begin{itemize}
    \item \textbf{Data splitting:}
    \begin{itemize}
        \item Sample splitting is performed using different studies and biological states (cross-dataset/batch splitting, rather than simple splitting from the same source).
        \item 30\% of the training set is further divided into a validation set.
    \end{itemize}
    
    \item \textbf{Loss function and optimizer:}
    \begin{itemize}
        \item Cross Entropy loss function is used.
        \item SGD optimizer is used.
        \item Cosine learning rate decay is introduced for the learning rate.
    \end{itemize}
    
    \item \textbf{Training period:} Convergence within 20 epochs.
\end{itemize}
\end{frame}

% \begin{frame}{Blocks of Highlighted Text}
%     In this slide, some important text will be \alert{highlighted} because it's important. Please, don't abuse it.

%     \begin{block}{Block}
%         Sample text
%     \end{block}

%     \begin{alertblock}{Alertblock}
%         Sample text in red box
%     \end{alertblock}

%     \begin{examples}
%         Sample text in green box. The title of the block is ``Examples".
%     \end{examples}
% \end{frame}

%------------------------------------------------

% \begin{frame}{Multiple Columns}
%     \begin{columns}[c] % The "c" option specifies centered vertical alignment while the "t" option is used for top vertical alignment

%         \column{.45\textwidth} % Left column and width
%         \textbf{Heading}
%         \begin{enumerate}
%             \item Statement
%             \item Explanation
%             \item Example
%         \end{enumerate}

%         \column{.45\textwidth} % Right column and width
%         Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.

%     \end{columns}
% \end{frame}

%------------------------------------------------
\section{A foundation model of transcription across human cell types (GET)}
%------------------------------------------------


\begin{frame}
\begin{center}
    \Large \textbf{A foundation model of transcription across human cell types}
    
    \vspace{1cm}
    
    \normalsize Fu X, Mo S, Buendia A, Laurent AP, Shao A, Alvarez-Torres MDM, Yu T, Tan J, Su J, Sagatelian R, Ferrando AA, Ciccia A, Lan Y, Owens DM, Palomero T, Xing EP, Rabadan R
    
    \vspace{0.5cm}
    
    \normalsize \textit{Nature}, 2025
    
    \vspace{0.3cm}
    
    \small DOI: 10.1038/s41586-024-08391-z
    
    \vspace{0.5cm}

    \footnotesize \href{https://www.nature.com/articles/s41586-024-08391-z.pdf}{https://www.nature.com/articles/s41586-024-08391-z.pdf}
\end{center}
\end{frame}

\begin{frame}{Introduction}

In genearl, GET is a probalistic model aiming to predict $p(E|X,C)$ where:
\begin{itemize}
    \item $E$: Gene expression levels (response variable)
    \item $X$: Regulatory feature matrix (chromatin accessibility + transcription factor binding sites)
    \item $C$: Cell type conditions (even many other conditions, e.g. different experiment methods, sequencing platforms, physiological states, etc.)
\end{itemize}

It adopts a two-stage framework: pre-training and fine-tuning. 
\begin{itemize}
    \item \textbf{Pre-training:} In pretraining, it learns the distribution pattern of regulatory features given the cell types $p(X|C)$ with unsupervised learning.
    \item \textbf{Fine-tuning:} Adapting the pre-trained model to specific cell types to predict $f\circ p: X\to E$.
\end{itemize}

The ultimate goal is to generalize the model's predictions across different cell types and conditions:
$$\min_\theta \mathbb{E}_{C^{\text{test}}}  \left[ \mathcal{L}(f_\theta(X^{\text{test}}), E^{\text{test}}) \right] $$

\end{frame}

%------------------------------------------------

\begin{frame}[allowframebreaks]{Notation}

\begin{itemize}
    \item For a gene $g$ in cell type $c$, its regulatory region window (2 Mbp long) can detect $N$ regions/peaks, denoted as $\{r_i\}_{i=1}^N$ (here $N = 200$).
    
    \item For each region $r_i$:
    \begin{itemize}
        \item We perform motif analysis to obtain motif scores $\boldsymbol{m}_i \in \mathbb{R}^{d_m}$, where $d_m = 282$ is the motif dimension.
        \item We have chromatin accessibility information $a_i \in \mathbb{R}$, measured through scATAC-seq experiments using logCPM counts.
    \end{itemize}
    
    \item We concatenate motif scores and accessibility to obtain the feature representation: 
    $$\boldsymbol{x}_i = \begin{bmatrix} \boldsymbol{m}_i \\ a_i \end{bmatrix} \in \mathbb{R}^{d}$$
    where $d := d_m + 1 = 283$.
    
    \item All $N$ regions are encoded to obtain the input feature matrix:
    $${X} = \begin{bmatrix} \boldsymbol{x}_1^\top \\ \boldsymbol{x}_2^\top \\ \vdots \\ \boldsymbol{x}_N^\top \end{bmatrix} \in \mathbb{R}^{N \times d}$$
\end{itemize}
\end{frame}


\begin{frame}{Key Architecture: Region Embedding}
    For each region $r_i$, we first transform its feature vector $\boldsymbol{x}_i$ into a higher dimensional space through linear transformation (without activation):

    \begin{itemize}
        \item Original feature matrix: ${X} \in \mathbb{R}^{N \times d}$
        \item Linear transformation: $W_{\text{Emb}} \in \mathbb{R}^{d \times D}$, where $D = 768$ in the paper
        \item Transformed feature matrix:
        $$X' := \text{RegionEmb}(X) = X W_{\text{Emb}} \in \mathbb{R}^{N \times D}$$
    \end{itemize}

    Note: Since motifs in the original vectors have high correlation, nonlinear transformations are avoided early to prevent compression/interference.

\end{frame}


\begin{frame}[allowframebreaks]{Key Architecture: Token-wise Self-Attention}

To model regulatory relationships between peaks (including cis-interactions and trans-interactions), we employ a 12-layer Transformer Encoder with standard self-attention mechanism:

\begin{itemize}
    \item Input: Region embeddings $X' \in \mathbb{R}^{N \times D}$ from previous layer
    \item For each attention head:
    \begin{itemize}
        \item Linear projections to Query, Key, Value spaces:
        $$\begin{aligned}
        Q &= X' W_q \in \mathbb{R}^{N \times d_k} \\
        K &= X' W_k \in \mathbb{R}^{N \times d_k} \\
        V &= X' W_v \in \mathbb{R}^{N \times d_v}
        \end{aligned}$$
        where $W_q, W_k \in \mathbb{R}^{D \times d_k}, W_v \in \mathbb{R}^{D \times d_v}$ are learnable
        \item Compute scaled dot-product attention:
        $$O_h = \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$$
    \end{itemize}
    
    \item Multi-head processing:
    \begin{itemize}
        \item Compute multiple attention heads in parallel 
        \item Concatenate outputs and project through fully connected layer
        \item Add residual connections and layer normalization
        \item Include feed-forward layers between attention blocks
    \end{itemize}
\end{itemize}

The 12-layer architecture enables modeling complex interactions between regulatory regions.

\end{frame}

\begin{frame}[allowframebreaks]{Training Procedure: Pre-training}
The pre-training phase employs a \textbf{masked autoencoding} strategy, inspired by models like BERT, to learn meaningful representations of genomic regions.

\begin{itemize}
    \item \textbf{Masking:} For each input matrix $X \in \mathbb{R}^{N \times d}$, half of the regions are randomly selected for masking. Let $\mathcal{M}$ be the set of indices for these masked regions, where $|\mathcal{M}| = N/2$.
    \begin{itemize}
        \item The feature vectors $\boldsymbol{x}_i$ for all masked regions ($i \in \mathcal{M}$) are replaced by a single, learnable mask token vector $\boldsymbol{m} \in \mathbb{R}^d$. This results in a masked input $X^{\text{masked}}$.
    \end{itemize}
    
    \item \textbf{Reconstruction Task:} The model is trained to reconstruct the original feature vectors of the masked regions from the corrupted input.
\end{itemize}

\vspace{0.5cm}
The training process is as follows:
\begin{enumerate}
    \item The masked input $X^{\text{masked}}$ is passed through the Region Embedding layer to get token embeddings:
    $$H^{\text{masked}} = \text{RegionEmb}(X^{\text{masked}}) \in \mathbb{R}^{N \times D}$$
    
    \item The embeddings are processed by the Transformer Encoder to produce contextualized representations:
    $$Z^{\text{masked}} = \text{Transformer}(H^{\text{masked}}) \in \mathbb{R}^{N \times D}$$
    
    \item A linear decoder head uses the output representations $\boldsymbol{z}^{\text{masked}}_i$ to predict the original features for the masked regions:
    $$\boldsymbol{\hat{x}}_i = \boldsymbol{z}^{\text{masked}}_i W_{\text{dec}} \in \mathbb{R}^{d}, \quad \forall i \in \mathcal{M}$$
    
    \item The loss function is the reconstruction error (e.g., Mean Squared Error) between the predicted and original features for the masked regions:
    $$\mathcal{L} = \sum_{i \in \mathcal{M}} \| \hat{x}_i - x_i \|_2^2$$

    Abstractly, the goal is to train the encoder $p$ and a prediction head $g$ to minimize the expected reconstruction error over all possible data samples and masks:
$$\min_{p, g} \ \mathbb{E}_{X, \mathcal{M}} \left[ \sum_{i\in\mathcal{M}} \left\| g(p(X^{\text{masked}}))_i - x_i \right\|^2 \right]$$
\end{enumerate}


\break

The training details in this paper are as follows:
\begin{itemize}
    \item \textbf{Optimizer:} AdamW with a weight decay of 0.05
    \item \textbf{Batch Size:} 256
    \item \textbf{Epochs:} 800, with a 40-epoch linear warmup period
    \item \textbf{Max Learning Rate:} $1.5 \times 10^{-4}$
    \item \textbf{Compute:} Trained for 7 days on 16 NVIDIA V100 GPUs
\end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}[allowframebreaks]{Training Procedure: Fine-tuning}
After extensive masked pre-training, we fine-tune the model for gene expression prediction tasks.

\begin{itemize}
    \item \textbf{Task:} Predict the RNA expression level $\hat{y}$ for a gene $g$ in cell type $c$, given its regulatory region features $X$.
    
    \item \textbf{Process:}
    \begin{enumerate}
        \item Use pre-trained encoder to obtain latent representations:
        $$H = p(X) = \begin{bmatrix} h_1^\top \\ \vdots \\ h_N^\top \end{bmatrix} \in \mathbb{R}^{N \times D}$$

        \item Apply attention pooling to aggregate region representations:
        $$z = \sum_{i=1}^N \alpha_i h_i \in \mathbb{R}^D, \quad \text{where } \alpha_i = \frac{\exp(w^\top h_i)}{\sum_{j=1}^N \exp(w^\top h_j)}$$
        
        \item Pass pooled representation through an MLP:
        $$\hat{y} = f_\phi (z)$$
    \end{enumerate}
    
    \item \textbf{Optimization objective:}
    $$\min_{\phi, \theta} \sum_{(g,c)} \mathcal{L}_{\text{expr}}\left(f_\phi(p_\theta(X_{g,c})), y_{g,c}\right)$$
    where $\mathcal{L}_{\text{expr}}$ is Poisson Negative Log Likelihood loss for count data.
\end{itemize}

Note: In implementation, this paper adopts LoRA \href{https://arxiv.org/abs/2106.09685}{(Hu et al., 2021)} for efficient parameter fine-tuning.  

\end{frame}

%------------------------------------------------
\section{VCC Baseline and Cell-eval}

\begin{frame}
\begin{center}
\Large{\textbf{VCC Baseline and Cell-eval}}

\vspace{1cm}

\footnotesize\href{https://github.com/ArcInstitute/cell-eval}{https://github.com/ArcInstitute/cell-eval}
\end{center}
\end{frame}


%------------------------------------------------
% \section{Deep-learning-based gene perturbation effect prediction does not yet outperform simple linear baselines}


% \begin{frame}
% \begin{center}
%     \Large \textbf{Deep-learning-based gene perturbation effect prediction does not yet outperform simple linear baselines}
    
%     \vspace{1cm}
    
%     \normalsize Ahlmann-Eltze C, Huber W, Anders S
    
%     \vspace{0.5cm}
    
%     \normalsize \textit{Nature Methods}, 2025
    
%     \vspace{0.3cm}
    
%     \small DOI: 10.1038/s41592-025-02772-6
    
%     \vspace{0.5cm}
    
%     \footnotesize \href{https://www.nature.com/articles/s41592-025-02772-6.pdf}{https://www.nature.com/articles/s41592-025-02772-6.pdf}
% \end{center}
% \end{frame}

% \begin{frame}{Abstract}
%     \begin{itemize}
%         \item This benchmark study challenges the claimed superiority of deep learning foundation models for gene perturbation effect prediction. 
%         \item The authors systematically compared five foundation models and two other deep learning approaches against deliberately simple baselines across multiple datasets. 
%         \item \textbf{None of the deep learning models outperformed simple linear baselines, despite significantly higher computational costs.}
%     \end{itemize}
% \end{frame}

% \begin{frame}{Models Compared}
%     \textbf{Foundation Models:}
%     \begin{itemize}
%         \item \textbf{scGPT}: Transformer-based model specifically designed for single-cell data
%         \item \textbf{scFoundation}: Large-scale pre-trained model for cellular representation learning
%         \item \textbf{UCE, scBERT, Geneformer}: Repurposed foundation models with linear decoders for perturbation prediction
%     \end{itemize}

%     \textbf{Other Deep Learning Approaches:}
%     \begin{itemize}
%         \item \textbf{GEARS}: Graph neural network that leverages gene-gene interaction information
%         \item \textbf{CPA}: Conditional variational autoencoder for perturbation effect modeling
%     \end{itemize}

% \end{frame}


% \begin{frame}[allowframebreaks]{Experiment: Simple Baselines Outperform Deep Models}
% The key settings of the experiment are as follows:
% \begin{itemize}
%     \item \textbf{Dataset:} 100 single + 124 double perturbations (K562 cells, CRISPR activation) (Norman et al.), with logarithm transformation preprocessing.
%     \item \textbf{Train-test Split:} 
%         \begin{itemize}
%             \item \textbf{Train:} All 100 single perturbations + 62 randomly selected double perturbations ($50\%$ of total) for training.
%             \item \textbf{Test:} Remaining 62 double perturbations for testing.
%             \item The splitting has been repeated randomly 5 times to ensure robustness.
%         \end{itemize}
%     \item \textbf{Baseline Design:} 
%         \begin{itemize}
%             \item \textit{No-change Baseline} (Tests whether any signal exists in the data):  $\hat{y}_{\text{perturbation}} = y_{\text{control}}$. 
%             \item \textit{Additive model} (Assumes independent gene effects, no epistasis): $ \hat{y}_{A+B} = \hat{y}_{A} + \hat{y}_{B}-\hat{y}_{\text{control}}$.
%         \end{itemize}
% \end{itemize}

% First, it examines the $L_2$ error between predicted and observed expression profiles:

% \break

% \begin{figure}
%     \centering
%     \includegraphics[width=0.93\textwidth]{fig/ExpRes.jpg}
%     \caption{\textbf{Double perturbation prediction}. \textbf{a,} Beeswarm plot of the  $L_2$ distance between the predicted and the observed expression profile of the $n = 1,000$ most highly expressed genes.  \textbf{b,} Scatterplots of observed versus predicted expression from one example of the $62$ double perturbations. The numbers indicate error measured by the $L_2$ distance and the Pearson delta ($R^2$).}
% \end{figure}
% \end{frame}


% \begin{frame}[allowframebreaks]{Experiment: Interaction Effect Detection Weakness}

% \begin{itemize}
% \item Then it applies hypothesis testing to test if the model can accurate detect genetic interaction effects. Specifically, it assumes if $|\hat Y_{\text{predicted}}(g,p) - Y_{\text{addictive}}(g,p) |> \delta$ for some threshold $\delta$, then the model has detected an interaction effect.
% \item The results shows that none of the models was better than the ‘no change’ baseline.
% \end{itemize}




% \begin{figure}
%     \centering
%     \includegraphics[width=0.35\textwidth]{fig/c}
%     \caption{\textbf{c, }TPR (recall) of the interaction predictions as a function of the false discovery proportion. FN, false negative; FP, false positive; TP, true positive.  }
% \end{figure}

% \break

% To further investigate, it tries to classify the types of interactions with : buffering, synergistic, and antagonistic effects.
% \begin{itemize}
%     \item All deep learning models tend to predict buffering effects more frequently than synergistic or antagonistic effects. 
%     \item It prefers that the interaction effects are usually  less than additive.
%     \item Models tends to be conservative in predicting interaction effects.
% \end{itemize}

% \break
% \begin{figure}
% \centering
%     \includegraphics[width=0.6\textwidth]{fig/de}
%     \caption{\textbf{d,} Schematic of the classification of interactions based on the difference from the additive expectation (the error bars show the additive range). \textbf{e,} Bar chart of the composition of the observed interaction classes.}
%     \end{figure}

% \break
% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\textwidth]{fig/f}
%     \caption{\textbf{f,} Top: scatterplot of observed versus predicted expression compared to the additive expectation. Bottom: mosaic plots that compare the composition of highlighted predictions from the top panel stratified by the interaction class of the prediction. }
% \end{figure}

% \end{frame}


% \break

% \begin{frame}{Experiment: Lack of Variation in GeneralizationPrediction}

%     Another problem is that the predictions of all models tend to underestimate the variation. 

%     \begin{figure}
%         \centering
%         \includegraphics[width=0.75\textwidth]{fig/ext.jpg}
%         \caption{Variation of the predicted and observed expression values. Histogram of the standard deviation per gene for the predicted and observed expression values across perturbations facetted by the model. The red vertical bar indicates the mean of the standard deviations for the ground truth.}
%     \end{figure}
% \end{frame}


% \begin{frame}[allowframebreaks]{Linear Decomposition Analysis}

% The authors further propose a linear model, using the embedding input from the deep learning models, but with a simple, low-rank linear approximation:
% $$
% Y \approx G W P^\top + b
% $$
% where:
% \begin{itemize}
%     \item $Y \in \mathbb{R}^{n_{\text{genes}}\times n_{\text{pert}}}$: Gene expression matrix, with $n_{\text{genes}}$ genes and $n_{\text{pert}}$ perturbations.
%     \item $G \in \mathbb{R}^{n_{\text{genes}}\times d_{\text{gene}}}$: Gene embedding matrix, obtained from the deep learning models.
%     \item $P \in \mathbb{R}^{n_{\text{pert}}\times d_{\text{pert}}}$: Perturbation embedding matrix, obtained from the deep learning models.
%     \item $W \in \mathbb{R}^{d_{\text{gene}}\times d_{\text{pert}}}$: Weight matrix to be optimized.
%     \item $b \in \mathbb{R}^{n_{\text{genes}}}$: Bias term for each gene, or the base expression level.
% \end{itemize}

% The objective is:
% $$
% \min_{W} \| Y - (G W P^\top + b) \|_{Frob}^2
% $$
% which yields the ridge regression solution:
% $$
% W^* = (G^\top G+\lambda I)^{-1} G^\top (Y_{\text{train}} - b) P (P^\top P + \lambda I)^{-1}.
% $$

% \break

% Then the authors conduct ablation studies to further investigate the model performance.
% \begin{itemize}
% \item It also includes the simpler model: the mean across the perturbations in the training set. 

% \item The results show that none of the deep learning models was able to consistently outperform the mean prediction or the linear model. 
% \item \textbf{The predictive value of complex models resides primarily in their learned representations, not their architectural complexity.}
% \item \textbf{Foundation model pre-training provides measurable but limited benefit over random initialization, while task-specific feature learning remains superior.}
% \end{itemize}


% \break


% \begin{figure}
%     \centering
%     \includegraphics[width=0.84\textwidth]{fig/2.jpg}
%     \caption{\textbf{Single perturbation prediction.} a, Beeswarm plot of the prediction errors for 134, 210 and 24 unseen single perturbations across two test–training splits (Methods). }
% \end{figure}

% \end{frame}


% \begin{frame}{Several Takeaways}
%     \begin{itemize}
%     \item \textbf{Baseline Design is Critical}. Always include deliberately simple baselines, as well-designed simple methods often reveal the true difficulty of tasks.
%     \item \textbf{Biology is More Linear Than Expected}. 96\% of gene pairs show additive effects (no interactions). 4\% show interactions (mostly buffering, rare synergy). Simple linear assumptions capture most biological reality.
%     \item \textbf{Task Relevance is More Important Than Data Scale}. Small perturbation datasets from similar systems $>$ Massive single-cell atlases from diverse conditions.
%     \item \textbf{The ``Flat Prediction'' Problem}. Deep learning models show severely reduced prediction variance; usually fail to capture biological diversity and perturbation specificity. 


%     \end{itemize}
% \end{frame}

%------------------------------------------------
%             \textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2} \\
%             \midrule
%             Treatment 1         & 0.0003262           & 0.562               \\
%             Treatment 2         & 0.0015681           & 0.910               \\
%             Treatment 3         & 0.0009271           & 0.296               \\
%             \bottomrule
%         \end{tabular}
%         \caption{Table caption}
%     \end{table}
% \end{frame}

%------------------------------------------------

% \begin{frame}{Theorem}
%     \begin{theorem}[Mass--energy equivalence]
%         $E = mc^2$
%     \end{theorem}
% \end{frame}



%------------------------------------------------

% \begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
%     \frametitle{Citation}
%     An example of the \verb|\cite| command to cite within the presentation:\\~

%     This statement requires citation \cite{p1}.
% \end{frame}

%------------------------------------------------

% \begin{frame}{References}
%     \footnotesize
%     \bibliography{reference.bib}
%     \bibliographystyle{apalike}
% \end{frame}

%------------------------------------------------


%----------------------------------------------------------------------------------------

\end{document}