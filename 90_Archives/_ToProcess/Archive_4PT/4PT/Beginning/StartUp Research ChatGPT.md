# 使用自定义编码手册和大型语言模型（LLM）进行文档分类

## 引言

利用大型语言模型（LLM）来自动化对学术论文进行分类，现在已成为一种可行的方法，即使是对于像Cashore的四种问题类型（4PT）这样的自定义分类框架。用户的目标是准确地将约10页的学术文章标记为4PT类别，这需要借助能够遵循详细指令（即4PT编码手册）并处理长文档的LLM。本报告探讨了2025年执行此任务的最先进的LLM解决方案，并提供了方法论和工具方面的指导。我们将涵盖：

-   领先的LLM模型（包括基于API和本地部署的模型），它们以强大的指令遵循能力和长上下文理解能力而闻名。
-   提示工程 vs. 微调 vs. 检索增强生成（RAG）——在已有标注数据集的情况下，针对此分类用例的优缺点分析。
-   用于大规模自动化分类流程的工具和平台（如LangChain、Haystack、DSPy等）。
-   评估策略，用于监控和提高分类准确性，包括不确定性估计方法。
-   如果决定为此任务微调LLM，我们将提供微调的最佳实践和推荐平台（如OpenAI微调、Hugging Face/LoRA等）。

在整个报告中，我们强调分类的准确性和可靠的指令遵循能力，并引用最新的研究发现和实例。

## 1. 用于长文档分类的最佳LLM模型

-   **OpenAI GPT-4——当前的黄金标准**：OpenAI的GPT-4被广泛认为是处理复杂语言任务最强大的模型之一。它在许多学术基准测试中表现出与人类相当的性能[1]，包括文本分类。GPT-4擅长遵循细致的指令，并能通过其扩展的上下文窗口（高达32k个令牌，约合24,000个单词或50页）处理长篇输入[2]。事实上，据预测，GPT-4能够“以高准确性将文本分类到不同主题，在许多数据集上超过人类水平”[3]。这使得GPT-4成为将长篇论文分类到专门类别中的首选——它可以在一次处理中消化整篇10页的文档并应用4PT标准。主要缺点是成本和延迟。GPT-4的API比小型模型昂贵得多，速度也慢得多[4]。对于大规模处理（数百或数千篇论文），费用会不断累积，特别是使用32k上下文版本时（其单位令牌成本很高[5]）。尽管如此，为了获得最高的准确性和指令遵循度，GPT-4（通过OpenAI的API）是一个领先的选择。

-   **Anthropic Claude 2（及Claude 2.1）——长上下文专家**：Anthropic的Claude模型以其极大的上下文窗口和强大的交互能力而闻名。Claude 2提供高达100k个令牌的上下文（约75,000个单词）[6]，这足以轻松容纳整篇学术论文加上编码手册的指令。Claude还通过“宪法AI”方法进行了微调，以便有益且安全地遵循指令。虽然Claude的原始推理能力略低于GPT-4，但它在长文本的摘要和分类任务上表现非常出色。一份报告指出，Claude 2的对话准确性相比早期模型有显著进步，尽管在“原始语言任务性能”上可能不及GPT-4[7]。在实践中，Claude 2常被拿来与GPT-3.5相媲美，并且更安全、更不容易偏离指南。对于我们的用例，只要在提示中给出4PT编码手册和指令，Claude就可以可靠地阅读一篇10页的论文并进行分类。事实上，Claude (v2.1) 在一个文本分类场景中显示出高召回率（意味着它能找到一个类别的大部分案例，尽管有时会牺牲精确度）[8]。Anthropic通过API提供Claude，使其可以用于自动化流程。**优点**：巨大的上下文窗口（无需对文档分块），良好的指令遵循能力，以及适中的成本。**缺点**：在某些评估中精确度略低于GPT-4[9]，并且目前只能通过云API使用（无法本地部署）。

-   **OpenAI GPT-3.5 Turbo (16k/32k)——快速廉价，但准确性较低**：GPT-3.5 Turbo是OpenAI推出的一个更便宜的替代品，提供16k版本，并且（截至2025年）可能有更大上下文的版本。它继承了ChatGPT的一般对话能力，但不如GPT-4细致。在分类任务中，只要提示设计得好，GPT-3.5可以做得不错，但通常在质量上“远逊于GPT-4”[10][4]。例如，在一项多类别分类测试中，GPT-3.5在顶级模型中获得了最低的准确率（57%）和F1分数（约48）[11]。其指令遵循能力对于直接的提示来说尚可，但对于像4PT这样复杂的框架，它有时可能会过度简化或混淆类别。主要优势是成本——GPT-3.5每个令牌的费用比GPT-4便宜约30倍[4]——以及速度。如果预算是主要考虑因素，并且可以接受略低的准确性，那么可以使用精心设计的提示配合GPT-3.5。但鉴于这里强调准确性，人们可能更倾向于GPT-4或微调模型。

-   **Google PaLM 2和Gemini模型——新兴的竞争者**：谷歌的PaLM 2大型语言模型（可通过Google Cloud Vertex AI访问，即text-bison）也能够理解长文本（上下文长度8k+令牌）并具有强大的多语言理解能力。到2024年底，谷歌推出了Gemini模型系列，其中Gemini Pro在分类任务中表现出色。在一项对支持工单进行分类的直接比较实验中，Gemini Pro在总体准确率和F1分数上优于GPT-4、Claude 2.1和GPT-3.5[9]。在该任务中，它实现了约74%的准确率，而GPT-4为70%，这表明谷歌的最新模型已经缩小了差距。Gemini的强劲表现表明它能很好地遵循领域特定的指令，并可能在精确度和召回率之间取得良好平衡[8]。然而，在撰写本文时，对Gemini的访问可能仅限于部分合作伙伴或服务（很可能通过谷歌的API），并且上下文长度或成本等细节仍在浮现。对于一个前沿项目，值得关注谷歌LLM产品的发展——特别是如果它们变得广泛可用，因为Gemini Pro在至少一次评估中提供了最佳的分类指标[9]。

-   **开源LLM（Llama 2及其他）——本地部署选项**：如果不希望使用基于API的解决方案，近期的开源LLM提供了可行的替代方案。Meta的Llama 2（特别是70B参数模型）在许多基准测试中表现出与ChatGPT (GPT-3.5) 相当的性能[12][13]。Llama 2-70B在某些事实性任务上甚至可以与GPT-4相媲美，在测试中达到约85%的准确率，质量与GPT-4相似，同时运行成本要低得多（如果硬件允许）[14]。它的默认上下文窗口为4k个令牌，但有技术可以扩展它（一些项目使用位置插值或ALiBi编码将Llama-2微调到16k或32k令牌的上下文[15][16]）。在本地（或私有服务器上）运行Llama 2 70B可以完全控制数据和隐私，但代价是需要大量的GPU资源（通常至少需要2-4个高端GPU或优化的量化才能运行）。还有一些较小的变体（13B, 7B），它们更容易部署，但若不进行微调，可能不足以完成复杂的分类任务。开源社区已经产生了许多指令微调的聊天模型（例如Llama-2-Chat、Mistral模型、Falcon 40B等），它们开箱即用地具有不错的指令遵循能力。其中，Llama-2-Chat 70B因在有益性/人类评估提示上“与其他LLM相当或更优”而脱颖而出[17]，这意味着它可以非常紧密地遵循指令。总而言之，一个微调过的开源模型可以是一个强大的分类器：例如，通过在4PT标注数据上训练Llama-2，可以得到一个在本地运行但能达到高准确率的模型（我们将在第5节讨论微调）。权衡之处在于开发工作量，以及可能需要在上下文长度上做出妥协（除非使用像MosaicML的MPT那样原生支持长上下文的架构[18]）。如果数据安全或每次预测的成本至关重要，一个为4PT微调的开源模型可能很有吸引力。否则，像GPT-4或Claude这样的API模型通常能为此分类任务提供更高的开箱即用准确性。

-   **总结**：为了在不进行额外训练的情况下获得最佳的单次（one-shot）准确性，GPT-4是一个安全的选择（特别是使用32k版本来输入整篇论文）[3]。Claude 2提供了一个长上下文的替代方案，对于大输入可能同样出色且更便宜。谷歌的最新模型（Gemini）可能在特定任务上胜过其他模型，但可用性是一个因素。通过一些努力，微调的开源模型（如Llama-2）可以达到有竞争力的性能，同时允许本地部署。在实践中，通常会采用混合方法：例如，用GPT-4进行原型设计以评估性能，如果满意，要么在部署时继续使用它，要么将该能力“蒸馏”到一个微调的更小模型中以提高成本效益[19][10]。接下来，我们将探讨如何通过提示、微调或RAG来利用这些模型，以及每种方法的权衡。

## 2. 提示工程 vs. 微调 vs. RAG：优缺点

使用LLM进行自定义框架分类有多种方法：你可以提示一个通用模型来遵循你的指令（零次提示或少次提示），你可以在你的标注数据上微调一个模型，或者你可以结合检索技术。每种方法都有其优缺点，特别是考虑到你确实有一个用于4PT方案的标注数据集。下面我们比较这些策略：

### 提示工程（上下文提示）

-   **它是什么**：精心设计一个输入提示，指示LLM执行分类任务。这可能包括对4PT类别的描述（来自编码手册），以及可能带有标签的文本示例（少次提示示例），所有这些都与文档文本一起提供给模型。模型的参数不会被更新——我们依赖于预训练模型的知识和推理能力。
-   **优点**：
    -   **无需训练**——实现和迭代速度快。你可以通过编写一个合适的提示立即得到结果。与微调相比，这节省了时间和资源[20][21]。如果提示设计得好，即使是像GPT-4这样强大的零次提示模型，也常常能仅凭其通用理解能力在自定义分类任务上取得高准确率。例如，研究人员注意到，GPT-4加上精心设计的提示，有时在某些任务上的表现可以媲美专门的微调模型[22][23]。
    -   **充分利用大型模型的全部能力**：通过直接使用GPT-4或Claude，你可以从它们在多样化数据上的大规模训练中受益。它们可能会推断出文本中与你的类别相对应的微妙线索，即使你的编码手册是领域特定的。事实上，“如果你能将一个问题翻译成英语，GPT-n就能提供好的结果——通常比为该任务训练的专门模型更好”[24]。这说明了顶级LLM惊人的零次/少次提示能力。
    -   **易于修改指令**：如果分类标准或编码手册更新了，你只需更改提示即可。无需重新训练模型——这使其对于不断变化的需求或多个分类方案非常灵活。
    -   **适用于封闭API**：提示是利用像GPT-4这样的模型通过API的唯一方式（截至2025年，GPT-4的微调是受限/封闭的）。因此，提示工程是充分利用这些最新模型能力的途径。
-   **缺点**：
    -   **提示敏感性和可变性**：LLM可能很挑剔——措辞的微小变化会影响结果。要大规模地获得一致的输出，可能需要反复试验和优化提示。这可能很脆弱；正如一位从业者所说，“提示工程适用于快速修复，但在需要一致输出的大规模部署中可能会变得难以管理”[25][26]。如果提示被误解，或者文档很长而提示没有强调正确的细节，模型可能会错误地遵循指令。确保模型每次都严格使用4PT标准可能具有挑战性——如果提示不够清晰，它有时可能会“自由发挥”。
    -   **每次运行的成本**：为每个文档提示一个大型模型可能很昂贵。学习成本无法分摊——模型每次都从提示中“重新学习”如何应用编码手册。例如，在每个请求中包含一个冗长的编码手册和示例会增加令牌数量。如果你要分类数千篇论文，那些提示令牌的成本可能相当可观。如果你计划大规模地重复运行推理，微调（一次性）可能更具成本效益。
    -   **上下文长度限制**：虽然像GPT-4 32k和Claude 100k这样的模型减轻了这个问题，但许多模型仍有局限性（例如，GPT-3.5最多约16k令牌，Llama-2 chat约4k）。一篇10页的论文加上详细的说明可能会接近这些限制。如果组合输入超过了模型的窗口，你就需要截断或总结文档——这会增加复杂性，并且除非小心处理，否则可能会损害准确性（这开始类似于RAG方法）。
    -   **缺乏保证的一致性**：因为模型不是专门化的，它有时可能会以高置信度产生一个不正确的标签，特别是当文本模棱两可或介于类别之间时。没有经过微调，模型没有明确学习从文本到4PT标签的映射；它是基于指令和通用知识进行即兴发挥。这可能导致细微的错误，尤其是在人类编码员通过仔细应用编码手册可能捕捉到的边缘案例上。

**总结**：提示工程是开始的最快方法，并且可以产生非常好的结果，特别是使用顶级模型时。它非常适合测试可行性，如果准确性足够且能负担API调用费用，甚至可以用于生产。它还允许使用最好的模型（GPT-4），这在某些情况下可能使微调变得没有必要[27]（一些研究发现GPT-4非常强大，以至于微调或单次示例在某些任务中“没有带来增量效益”[28]）。然而，如果需要极致的准确性和一致性，或者你想降低每个文档的成本，你可能会考虑微调或其他策略。

### 微调（在标注数据上进行监督训练）

-   **它是什么**：你使用你的标注论文数据集（带有它们的4PT类别）来进一步训练或调整一个LLM。这可能意味着完整的模型微调（更新所有权重），或者在开源模型上进行参数高效的微调，如LoRA（更新少量额外的权重）。或者，它可能意味着使用像OpenAI的微调API这样的平台，在分类示例上训练GPT-3.5（OpenAI的微调实质上是训练一个新的模型版本，以便在给定输入时产生期望的输出）。目标是创建一个能够内在学习从文本到4PT类别映射的模型，而不是每次都从提示中重新弄清楚。
-   **优点**：
    -   **提高准确性和一致性**：微调可以“教会模型新的行为”，使其深刻理解任务[29]。因为模型权重会调整以最小化你的数据上的分类错误，所以它通常在该任务上比单独使用提示获得更高的准确性。如果你的编码手册标准很复杂，这一点尤其正确——模型可以从训练数据中学习到微妙的模式。从业者报告说，微调GPT-4（或3.5）可以将其变成一个领域专家，在小众任务上带来“天壤之别”的改进[30]。对于分类任务，微调往往能“极大地提升性能……其质量的提升程度坦白说是惊人的”，正如一个团队在微调一个GPT-3.5分类器以超越GPT-4的质量时发现的那样[31]。本质上，一个微调过的模型会更可靠，并且需要更少的提示工程——它看到输入时就知道该做什么。
    -   **重复使用的效率**：一旦训练完成，一个微调过的模型每次查询的成本可以大大降低。例如，你的微调模型可能只需要接收论文文本并输出一个标签，而不是每次都发送一个带有指令的庞大提示。如果使用OpenAI，微调模型（基于GPT-3.5）的单位令牌成本比GPT-4低，而且你不需要在提示中包含冗长的指令。如果使用开源模型，在你自己的GPU上运行推理，在大规模应用时可以非常有成本效益。例如，Harriet团队用大约30美元微调了一个GPT-3.5模型，获得了GPT-4级别的分类准确性，然后享受着快速、廉价的预测[19][24]。这对于可扩展性至关重要。
    -   **领域和语气的适应**：微调不仅仅是为了准确性，它还让你能够调整模型的响应方式。如果你希望模型解释其推理过程或在其输出中使用特定的术语，你可以在训练输出中包含这些内容。例如，你可以微调它为每个输入生成一个包含字段的JSON（{"paper_id": X, "predicted_4PT": Y, "confidence": Z}）——这样模型的格式和风格就为你量身定制了。它还可以内化决策制定的语气（比如，如果你的训练数据包含这一点，它可能会总是引用一句话来证明分类的合理性）。这种级别的控制很难仅通过提示来实现。正如一位专家所说，“微调是你为你的特定需求解锁模型全部潜力的方式……提高准确性、格式一致性，以及处理提示工程通常无法管理的边缘案例”[32][29]。
    -   **分类头方法无上下文限制问题**：如果你在一个LLM或嵌入之上微调一个分类器，你可能可以规避推理时的严格上下文长度限制。例如，你可以微调一个像LongFormer这样的模型或一个本身就支持长文档的开源LLM变体，或者使用一种分层方法（微调以分类块并聚合）。此外，如果使用嵌入+分类器（一种模型驱动的RAG形式，见下文），你可以通过在嵌入前将长文本分块来处理任意长度的文本。
-   **缺点**：
    -   **需要足够的数据和精力**：微调只有在你拥有大量高质量的标注示例时才有效。如果数据集很小，大型LLM可能会过拟合，或者改进不大。 (尽管像少次微调或数据增强这样的技术可以提供帮助。) 本案例中的用户确实有一套标注的论文，但我们必须考虑这到底是几十个例子还是几千个——越多越好。微调还需要一些机器学习专业知识：以正确的格式准备训练数据，选择超参数或使用合适的工具，然后验证模型。它不像提示工程那样立竿见影。你需要进行评估和迭代，以避免性能下降（例如，不当微调一个大型模型有时会使其失去一些通用能力或变得过于狭隘）。简而言之，这是一个需要仔细准备数据集和训练策略的“艺术与科学的结合”[33][32]。
    -   **训练的资源和成本**：如果使用OpenAI的微调服务，训练该模型有单位令牌的成本（并且可能需要等待作业完成）。如果自己使用开源模型进行微调，你需要大量的GPU资源来微调一个大型模型（尽管像LoRA这样的方法可以减少内存需求）。还有机会成本：与其花时间微调，不如短期内直接付费使用GPT-4。对于一个论文数量不多的一次性项目，微调可能不值得。当你要重复分类大量文档时，它才能发挥优势。
    -   **训练后灵活性降低**：一个微调过的模型被固定于它所见过的任务和数据。如果编码手册或定义发生变化，模型需要重新训练，或者至少需要仔细的提示来调整。相比之下，使用基于提示的方法，你可以随时更新指令。还有一个风险是，微调模型可能会从训练集中学到偏见；例如，如果训练集中大多数“类型1”问题的论文恰好都来自某个特定的期刊，模型可能会抓住一些不相关的特征。稳健的训练和正则化可以减轻这个问题，但这是一个需要关注的问题。使用基础模型进行提示会利用模型更广泛的知识，这有时能更好地泛化到新的案例，而微调模型可能会过拟合于过去类别的样子。
    -   **封闭模型的局限性**：并非所有顶级模型都允许微调。截至2025年，OpenAI允许在GPT-3.5 Turbo上进行微调（并已开始有限的GPT-4微调），但有一些限制（例如，微调GPT-4可能非常昂贵且不广泛开放）。Claude在早期版本中没有向第三方提供微调服务。所以你的微调解决方案可能使用的是一个稍低级别的模型（如GPT-3.5或一个开源LLM）。这仍然可以非常出色——例如，一个微调的GPT-3.5分类器可以超越提示的GPT-4，正如Harriet的实验所示[19]——但这是需要考虑的一点。

**总结**：当你拥有一个大量的标注集，并且需要最高的准确性、效率和一致性时，推荐进行微调。它实质上是将编码手册的知识“烘焙”到模型中。一个最佳实践是结合策略：“使用提示工程以获得灵活性，使用微调以获得可靠性”[26]。例如，你可以提示GPT-4生成额外的训练示例或解释（利用其优势），用这些来微调一个更便宜的模型，然后部署微调模型以进行扩展。微调在大规模流程化使用（大量文档）以及当你需要模型处理单独提示难以处理的边缘案例或特定格式时表现出色。

### 检索增强生成（RAG）

-   **它是什么**：RAG通常涉及使用外部知识库或在文本语料库上进行搜索，以将相关信息提供给LLM的提示。在分类的上下文中，“RAG”可能意味着几件事：
    1.  **检索编码手册或指南**：与其将整个4PT编码手册塞进每个提示中，你可以将其各部分存储在向量数据库中，并检索与当前文档相关的部分（例如，检索四种类型的定义，或特定的区分标准）以包含在提示中。这确保模型总是有指令，而不会耗尽上下文或为不相关的令牌付费。
    2.  **检索相似示例**：有了一个标注数据集，你可以使用嵌入相似性搜索来找到与输入论文最相似的训练论文，并将它们作为范例包含在少次提示中。这样，模型就能看到具体的类比（“论文A具有特征X，被标记为类型3”），这可以指导它处理新论文。这就像通过检索进行基于案例的推理。
    3.  **对输入进行分块和聚合**：如果一个文档非常长（比如超出了模型的上下文），可以将其分成多个块，让一个LLM分析/分类每个块，然后通过一个最终步骤来确定整体标签。例如，如果一半的块倾向于类型2，一半倾向于类型4，你可能需要一个规则或另一次LLM调用来决定整个文档应该得到哪个标签。这更像是一个流程，而不是经典的“RAG”，但使用了相似的原则（处理片段并组合）。
-   **优点**：
    -   **处理非常长的文档**：如果论文比LLM的上下文还长，RAG（特别是分块和检索）是一个解决方案。即使是当前32k+令牌的模型，一些文档（或一批文档）也可能超过这个限制。RAG方法可以，例如，使用一个包含论文所有段落的向量存储，并检索出决定问题类型的最相关的段落。如果4PT分类可以从某些关键部分（比如论文的引言和结论）推断出来，你可以让一个嵌入模型挑选出这些部分供LLM阅读。这确保了不会因为上下文限制而错过重要信息。
    -   **效率**：不是每次都输入整篇论文，你只检索模型需要看的内容。这可以显著减少令牌消耗。例如，如果模型只需要每篇论文的问题陈述部分来分类，检索器可以提取那部分，LLM可能只需要阅读1000个令牌而不是5000个。对于许多文档来说，这是节省成本的。
    -   **聚焦模型**：通过检索相关的片段或指令，RAG可以减少混淆。例如，如果某些句子强烈表明是类型1而不是类型2，检索器会将这些句子呈现在提示中，使模型的工作比扫描全文更容易。同样，检索“类型3”的精确编码手册定义并包含在提示中，意味着模型不太可能混淆它——它每次都有准确的参考。本质上，RAG可以注入专门的知识（编码手册）或引导模型的注意力到文本的正确部分。如果模型可能会幻化出标准或忽略微妙的线索，这会很有用。
    -   **保持最新的参考资料**：如果分类标准或示例发生变化，你只需更新知识库（编码手册文档或示例索引）——无需重新训练模型。LLM仍然是一个通用的推理引擎，而检索到的数据提供当前的真相。在某些情况下这可能更安全；例如，如果增加了新的问题类型，你的检索机制可以为提示获取这些新定义。
-   **缺点**：
    -   **增加了复杂性**：构建一个RAG流程更为复杂。你需要设置一个嵌入模型、一个向量数据库，可能还需要编写插入检索信息的提示模板，并处理结果的聚合。对于分类任务，一个直接的单步LLM调用（提示或微调）可能就足够了，所以引入RAG可能是过度设计，除非是出于上下文长度的原因。
    -   **检索错误**：如果你的检索器失败了（例如，检索到不具代表性的示例或错过了文本的关键部分），它可能会误导LLM。最终的准确性就取决于检索步骤的质量。存在一个风险，即论文的一个相关部分因为与查询不够相似而没有被检索到，导致模型因信息缺失而错误分类。
    -   **近似最近邻问题**：向量搜索并不完美；它们可能会检索到表面上相似但对分类没有真正帮助的文本。管理嵌入质量，并可能在检索中包含一些关键词或元数据是很重要的。这种调优是一个额外的负担。
    -   **延迟**：RAG流程引入了额外的步骤——嵌入每个文档或块，执行查询。在分类时，在模型生成输出之前，你可能会有轻微的检索延迟。对于大规模的批处理分类，这通常没问题，但它不像通过微调模型单次处理那样即时。
    -   **维护知识库**：如果使用基于示例的检索，你需要维护你的示例索引。如果数据集增长或你有了新的标注案例，你需要更新它。这不是大问题，但需要维护。

在4PT分类的背景下，RAG方法可能很有用，比如说，如果论文非常长，或者你想依赖与已知示例的相似性。然而，如果你的LLM有足够大的上下文（32k或100k），你可能仅仅为了处理长度而不需要RAG。在提示中包含编码手册文本也是可行的（4PT编码手册可能只有几页文本；这在大型模型的窗口内）。因此，一个更简单的路径可能是：使用包含/学习了编码手册的提示或微调，而不是动态检索其片段。

这里一个巧妙的检索用法可以是少次示例检索。例如，你可以计算所有训练论文摘要的嵌入，并在运行时检索与新论文最相似的前2个，然后提示：“论文X有[摘要]并被标记为类型2；论文Y有[摘要]并被标记为类型4。现在请分类这篇新论文。”这为模型提供了具体的类比。如果模型能从示例中受益，这项技术可以提高准确性，并且比手动为提示编写示例更具可扩展性[34]。缺点是需要确保检索到的示例是正确的并且不会令人困惑（糟糕的示例如果“不是预期数据的良好代表，可能会迷惑模型”[35]）。

**总结**：用于分类的RAG是一种可选的增强功能：它可以帮助处理极端的上下文大小或注入最新的参考数据。但它增加了复杂性。许多分类场景仅通过提示或微调就能成功。如果你发现模型难以区分类别，你可以尝试检索相似案例来引导它。或者如果文档太大，使用检索来提供摘要或相关部分。否则，如果使用GPT-4-32k或Claude-100k，你可能可以直接输入所有内容，而无需额外的检索层。

**混合方法**：
这些方法并非相互排斥。在实践中，你可能会将它们结合起来：例如，你可以微调一个模型，但仍然使用提示指令来完善其输出或要求其给出理由（微调用于核心分类能力，提示用于格式化）。或者你可以使用检索来为微调模型提供额外信息。一种有趣的方法是思维链提示与自洽性：在决定之前，提示模型通过推理过程进行分类（可能列出每个类别的优缺点），并多次这样做。这不完全是RAG，而是一种提示技术。研究表明，使用自洽性解码——生成多个输出并取多数票——可以提高推理任务的准确性[36]。对于分类，可以要求模型以5种不同的方式思考决策（带有一些随机性），看看哪个标签在输出中出现最频繁，以此来减少随机错误。当然，这会随着尝试次数的增加而线性增加成本，所以这是一个权衡，但它是最大化正确性的又一个工具。

**建议**：从使用你拥有的最佳模型（例如GPT-4或Claude）进行提示工程开始，以确定基线性能。如果准确率高（比如>90%）且一致，那可能就足够了。如果不够，并且你有足够的数据，微调一个模型（即使是较小的模型）很可能是提高准确性的下一步[32][31]。主要在面临上下文大小问题或想要整合动态知识时使用RAG或检索。鉴于对准确性的高度重视，一个微调模型或一个带有自洽性检查的精心提示的GPT-4可能会产生最佳结果。

## 3. 自动化分类流程的工具和平台

当大规模部署基于LLM的分类系统时，你会希望其结构是可重复、可维护和高效的。已经出现了一些框架和工具来帮助协调LLM并将其集成到数据流程中。这里我们重点介绍一些相关的工具，以及它们如何在这种场景下提供帮助：

### LangChain

LangChain是一个流行的用于构建LLM驱动应用程序的Python框架。它提供了管理提示、模型调用和操作链的抽象。对于一个分类流程，LangChain可以用于：
-   **通过统一的接口连接到LLM API或本地模型**。例如，你可以通过改变几行代码轻松地在OpenAI的GPT-4和一个开源模型之间切换。
-   **提示模板**：LangChain允许你定义带有占位符的系统和用户提示。你可以创建一个模板，自动插入文章文本和可能的类别定义列表[37][38]。这确保了你每次提示的一致性。
-   **链/流**：你可能会创建一个简单的链：文档 -> 提示 -> LLM -> 解析输出。LangChain可以帮助解析LLM的响应（例如，期望一个带有类别的JSON）。在一个教程示例中，LangChain的ChatOpenAI被用来将文章分类并一步生成JSON输出[39]。开发者设置了一个提示，指示模型输出一个带有"category"字段的JSON，LangChain处理了传递文章内容和接收结构化结果的过程。这种提示链在LangChain中非常直接[40]。
-   **批处理和集成**：使用LangChain，你可以遍历一个文档列表并应用相同的链。它还与数据加载器（如PDF加载器）和文本分割器集成。例如，如果你的论文是PDF格式，LangChain可以使用Unstructured加载器或PyPDF来提取文本[41][42]，然后输入到LLM链中。
-   **日志和监控**：LangChain有回调/钩子系统来记录令牌使用情况、模型响应等。这对于跟踪成本和调试输出很有用。在那个分类示例中，他们甚至展示了如何通过OpenAI API计算处理多篇文章的确切令牌使用量和成本[43][44]。这有助于监控开销和性能。

总的来说，如果你想快速原型设计并希望灵活地更换组件，LangChain是一个方便的选择。当你有多个步骤的工作流时（例如，一个检索示例的步骤，一个调用LLM的步骤，一个保存结果的步骤），它的优势就显现出来了。即使对于一个简单的“分类并保存”流程，LangChain也可以减少样板代码。然而，需要注意：对于非常大规模的作业，原生代码或批处理API调用可能更高效。LangChain正在不断改进以适应生产环境，其0.**版本也具有并发和异步调用的功能。

### Haystack (deepset)

Haystack是另一个框架，最初专注于构建带有检索器和阅读器的问答系统，但现在已经发展到支持包括生成式LLM在内的流程。其设计围绕着组件的流程（可以是分类器、检索器、生成器等）[45][46]。对于分类任务，你可以配置一个包含以下组件的Haystack流程：
-   一个**文档加载器**（用于加载文本），
-   （可选）一个**预处理器**（用于分割或清理文本），
-   一个调用模型的**LLM“生成器”**组件（或者如果使用经典模型，则是一个“分类器”组件），以及
-   一个将它们连接在一起的**流程**。

例如，Haystack有一个`DocumentClassifier`接口，可以包装一个Hugging Face文本分类模型来为文档分配标签[47]。更贴切的是，你可以在Haystack中使用`PromptNode`来调用LLM进行分类。你可以设置一个类似于LangChain的提示模板，并使用Haystack的流程在每个文档上运行它。如果需要，Haystack支持基于嵌入的检索器（`TransformersRetriever`等），所以你也可以用它来实现RAG方法（嵌入编码手册或示例，检索，然后提供给提示）。

Haystack的优势之一是流程分支的灵活性。它甚至有一个“路由器”的概念，可以将不同的输入定向到不同的组件。虽然对于单标签分类不直接需要，但这可以用于，比如说，你想先按语言路由文档（Haystack有一个`LanguageClassifier`节点[48]），然后再应用LLM。Haystack 2.0还引入了改进的流程编排，可以以DAG（有向无环图）风格运行，适合复杂的流程[49]。

然而，如果你的流程不涉及大量的检索或多步逻辑，Haystack可能就有些大材小用了。如果你计划与搜索引擎（它内置支持Elasticsearch, FAISS等）集成，并且你想通过REST API端点将流程产品化（Haystack为你的流程提供了开箱即用的REST API），它会非常有用。对于一个相对直接的纯分类作业，一些用户发现LangChain的方法或甚至简单的自定义脚本更容易。

**LangChain vs Haystack**：它们在功能上有所重叠（甚至可以一起使用[50]）。LangChain在LLM社区中因其快速原型设计而非常流行。Haystack在处理文档索引和检索方面更为成熟（鉴于其起源于问答系统）。如果你预见到需要RAG（比如存储许多示例向量或用于检索的块），Haystack可能是一个稳健的选择，因为它有优化的组件用于嵌入和查询，甚至支持大规模语料库的流式处理。如果只是没有检索的分类，LangChain可能感觉更轻量级。两者都是开源的，并且拥有不断增长的生态系统。

### LlamaIndex (GPT Index)

LlamaIndex，前身为GPT-Index，是一个专门设计用于连接LLM与外部数据的工具包。它通常用于文档问答，但也可以应用于分类。LlamaIndex有助于：
-   **文档索引和分割**：它可以摄入大型文档并创建索引（树索引、列表索引、向量索引等）。对于分类，你可以用它将每篇论文分解为可管理的节点，然后要么分类每个节点，要么使用索引帮助LLM找到答案（这里的“答案”是类别）。
-   **查询引擎**：你可以设置一个自定义查询，有效地询问“这份文件属于哪个4PT类别？”，然后让LlamaIndex处理从文档中检索相关信息并提供给LLM的过程。
-   这个库更偏向于问答或摘要任务，但创造性地使用可以将其重新用于分类——例如，可以给每个节点附加一个分类器函数并聚合结果。

虽然对于简单地标记每个文档来说，LlamaIndex可能有些 overkill（因为你已经有了文本，只需要一个标签），但如果你想做一些像交互式分类或提供理由的事情，它就变得很有用：例如，检索文本中对应于每个4PT维度的部分，以显示为什么该文档是类型4。LlamaIndex可以通过将文档组织成一个LLM可以导航的结构化形式来促进这种方法。

### DSPy

DSPy (Declarative Structured Python) 是一个较新的框架，专注于“编程，而不是提示”[51][52]。它允许你以声明性的方式定义一个LLM工作流：你指定输入和输出应该是什么，DSPy甚至可以帮助优化提示或在多个提示变体之间进行选择。这对于分类可能非常强大，因为：
-   **你可以正式地定义任务**：输入 = 文档文本，输出 = {类型1, 类型2, 类型3, 类型4}之一。
-   DSPy然后让你通过使用LLM提示或任何子工具的组合来实现这一点，你可以像测试普通函数一样测试它。
-   **它还支持自动提示优化**：例如，DSPy可以生成不同的提示措辞或使用少次示例，并评估它们以查看哪个能产生最佳的准确性[53]。随着时间的推移，它可以根据评估结果改进提示——一种自改进的流程[52]。对于关心最大化分类准确性的人来说，这个功能很有吸引力。你可以让DSPy搜索最优的提示或逻辑，而不是手动迭代提示。

DSPy也被构建为以模块化的方式与其他工具（如向量存储、外部API）集成[54]。它仍然是一个新兴的项目，所以使用它可能需要阅读其文档并可能调整其示例（它还没有像LangChain那样被广泛采用）。但它代表了更高层次编排的方向，即你专注于指定期望的行为（例如，“如果置信度低，调用另一个模型”或“如果需要，使用工具X获取更多信息”），而框架在底层处理提示调用。

### 其他需要考虑的工具：

-   **Hugging Face Transformers & Pipelines**：如果你微调一个模型（特别是开源模型），Hugging Face的`transformers`库是必不可少的。你可以用它来训练（使用`Trainer` API）然后部署模型。他们还有一个`TextClassificationPipeline`，可以包装一个模型以便于使用。此外，Hugging Face提供`Inference API`和`AutoTrain`平台——例如，你可以将你的数据上传到`AutoTrain`来微调一个分类器（也许使用像BERT或DistilBERT这样的小型语言模型作为基线），并将其与基于LLM的方法进行比较。如果你严格要求使用LLM，这可能超出了范围，但它是一个有用的基线，可以看到传统的微调模型表现如何（有时一个微调的SciBERT或类似模型在分类学术论文方面能提供很高的准确性）。
-   **批处理/并行处理工具**：如果数据集很大，像Ray或Dask这样的框架可以并行化对LLM的调用或嵌入计算。LangChain有一些对异步调用的集成，但如果从头开始做，你可能会使用Python的`asyncio`或`joblib`。此外，OpenAI的API支持在一个请求中批处理多个提示（对于某些端点，每个请求最多16个），这可以显著加快分类吞吐量——考虑在你的流程中使用它来减少开销。
-   **云平台**：对于部署，你可能会使用像Azure ML或GCP Vertex AI Pipelines这样的服务在生产环境中运行分类。这些服务可以调度和扩展作业，有些还原生支持OpenAI或其他LLM端点。例如，Azure的OpenAI服务可以为你托管一个微调模型，然后你可以在一个流程中集成它。

**结论**：工具的选择取决于你流程的复杂性。对于一个相对直接的场景（读取文档 -> 分类 -> 输出标签），你完全可以用裸的OpenAI Python SDK或`transformers`来编写脚本。然而，使用LangChain或DSPy可以让你更快地进行实验，并内置最佳实践（如稳健的提示和错误处理）。像Haystack或LlamaIndex这样的工具在你整合检索或需要更复杂的流程时会更有用。由于用户特别提到了这些工具，这表明他们对一个可扩展、可能是模块化的解决方案感兴趣——因此，探索LangChain的易于链接提示的特性[40]或Haystack的生产就绪流程[55]是值得的。这两个框架也支持日志记录和分析，这有助于评估，这也引出了下一个主题：如何评估和监控这个分类系统。

## 4. 评估策略和不确定性估计

确保分类的准确性并知道何时信任模型的预测至关重要。这里我们概述了如何评估模型的性能以及估计或处理不确定性的方法：

### 评估指标和设置

首先，你需要在一些保留的、已知4PT标签的测试集论文上，定量地测量LLM（无论是提示的还是微调的）的表现。标准的分类指标适用：
-   **准确率**：正确分类的论文百分比。如果每个类别同等重要，这是一个主要指标。
-   **每个4PT类别的精确率、召回率、F1分数**：由于4PT可能有些不平衡，或者你可能关心某些类型的错误（例如，将类型4误标为类型1可能比反过来更糟），因此检查每个类别的精确率（预测为类型1的中有多少是真正的类型1）和召回率（模型捕捉到了多少真正的类型1）是很好的。一个总体的宏观平均F1可以总结跨类别的性能。在一个例子中，评估者不仅通过准确率比较模型，还注意到了高精确率与高召回率的倾向（例如，GPT-4在一个支持工单分类任务中具有非常高的精确率但召回率较低[56]）。根据用例，你可能倾向于平衡（F1）或优先考虑其中一个。对于政策研究，你可能不希望任何类别被系统性地遗漏。
-   **混淆矩阵**：哪些类别经常被混淆。这可以突出显示模型是否经常将类型2和类型3互换，例如，这可能表明这些定义需要更清晰的提示或更多的训练数据来区分。

在评估LLM输出时，确保你有一个确定性的设置：对于提示，使用固定的`temperature=0`以保持一致性（这消除了随机性，所以你可以可靠地重现结果，模型答案不会漂移）。对于微调模型，如果可能，同样在确定性模式下进行评估。如果使用多个提示或CoT推理，评估可以包括多数投票（自洽性），如前所述，然后在最终决定的标签上计算指标。

### 定性错误分析

除了指标之外，还要仔细阅读一些错误分类的例子：
-   模型是否倾向于不成比例地输出某个特定类别？（也许它默认选择了最常见的类别——这是一个需要注意的偏见）。
-   失败的案例中是否存在模式？例如，也许它在处理类型4“棘手”问题上很吃力，因为这些问题需要识别紧迫性或路径依赖的微妙标准——也许提示或训练没有很好地传达这一点。识别这些模式可以指导提示的修订或针对这些案例的靶向微调。
-   利用模型的推理能力：你可以提示模型解释其分类（“用一句话说明为什么这篇论文是类型3”），看看这个推理是否与编码手册一致。在评估期间，让模型提供自我报告的理由可以帮助人类理解它是在正确的理由下做出的决定，还是抓住了错误的特征。（但要小心——模型有时可能会给出一个听起来合理但实际上并非导致其预测的理由，特别是如果没有明确要求它一步一步地思考）。

### 不确定性估计方法

与传统的输出概率或置信度分数的分类器不同，生成式LLM通常只输出一个标签。然而，有一些技术可以衡量不确定性：
-   **Logits/Logprob分析**：如果使用OpenAI的API，你可以请求补全令牌的`logprobs`。例如，你可以设计提示让GPT只输出一个单一的令牌（或一个单词）作为类别，并请求该令牌的`logprobs`[57][58]。返回的对数概率表明了模型对该选择的自信程度。越高的`logprob`（越接近0）意味着越高的置信度。Eric Jinks (2025) 在二元分类中演示了这一点，他找到了“true”与“false”令牌的概率，并将其用作模型置信度[59][60]。对于多类别分类，一种方法是明确让模型从一组固定的令牌中选择（例如，在提示中将类别标记为“Type1, Type2, Type3, Type4”，并获取这些令牌的`logprobs`）[61]。通过检查所选标签令牌的`logprob`，你可以得出一个伪置信度分数（例如，置信度 = exp(logprob_of_chosen)，这会给出一个介于0和1之间的概率）[62][63]。如果顶部令牌的概率只有，比如说，40%，而第二好的概率是35%，这就是一个低置信度的情况——模型不确定。如果顶部是95%，则非常确定。你可以设置一个阈值：低于某个置信度的分类，标记为需要人工审查或额外处理。这种`logprob`方法基本上是通过限制其输出空间，将生成模型当作分类器来对待。如果API支持，这是获得置信度的一种强大方式。注意：并非所有模型都允许获取logits（OpenAI对某些模型支持；开源模型如果你自己运行它们，可以获取logits）。
-   **多个提示或模型的共识**：另一个不确定性指标是一致性。如果你用不同的方式问模型同一个问题，或者使用不同的模型，它们是否都同意同一个标签？如果你实现了几个提示变体（或使用思维链 vs 直接提问，或同时使用GPT-4和Claude），并且它们都收敛到同一个答案，那么你对它的信心就更高。如果它们出现分歧，说明存在模糊之处。你可以通过运行，比如说，5个措辞略有不同的提示（或者使用`temperature`在~0.5时采样不同的推理路径）来看看答案的分布情况，从而量化这一点。这就是前面提到的自洽性方法：模型可能大多数时候给出一个答案，但如果在10次尝试中，它6次给出类型3，4次给出类型4，那么它就不是完全确定的。在这种情况下，你可以输出一个不确定性标志，甚至一个次优预测。研究表明，从多个推理样本中取多数票往往能提高准确性，并提供一个置信度度量（即多数票的大小）[36][64]。
-   **校准输出**：如果你有类似概率的分数（来自logits或投票频率），你可以使用像Platt缩放或保形预测这样的技术来校准它们，以便更好地映射到真实的正确率。例如，可以保留一部分数据，获取模型在其上的置信度分数，然后看看哪个分数对应于80%的准确率，并将其用作阈值。最近关于LLM不确定性量化的工作表明，像保形预测这样的方法可以提供覆盖保证（基本上是，“如果不自信到足以满足准确性保证，就保持未标记状态”）[65]。如果非常关键，你可能会探索那些学术方法以获得严格的不确定性度量。
-   **人机协同处理低置信度案例**：在实践中，一个好的策略是将不确定的案例转给人类专家。如果模型说“我在类型2和类型3之间只有50%的把握”，人类可以审查那篇论文。这样，你就可以将人力集中在模型处理困难的地方。随着时间的推移，你甚至可以用这些案例来扩充训练集（主动学习）。
-   **随时间监控**：一旦部署，继续记录模型的预测，并可能记录模型置信度或理由的样本。如果发生概念漂移（例如，突然你收到一批关于一种新问题的论文，让模型感到困惑），你会看到低置信度分数的增加或更多的人工干预。带有诸如被标记为不确定分类的百分比，或预测类别分布等指标的监控仪表板，可以提醒你出现问题。例如，如果突然90%的论文都被标记为类型1（而历史上这个比例是25%），那么模型或它接收的数据可能出了问题。
-   **与更简单的模型进行基准比较**：明智的做法是，同时有一个更简单的基线模型进行比较——也许是使用文本嵌入的逻辑回归或小型神经网络模型。如果LLM在某些输入上与基线模型差异很大，检查这些案例。有时基线模型可能会捕捉到明显的线索，而LLM则过度复杂化，反之亦然。分歧可以突出模糊的案例或模型错误。

**总结**：监控的最佳实践包括为每个预测捕获一个置信度指标（通过`logprob`或集成方法），并为人工审查设置一个阈值或策略。一个实现思路：当使用OpenAI的API时，请求top N个`logprob`令牌。如果顶部标签的概率低于，比如说，0.8，并且第二个标签紧随其后，你可以输出：“模型在X和Y之间不确定，需要审查。”OpenAI Cookbook中有一个以类似方式从`logprobs`中提取分类置信度的例子[66][67]。此外，在微调分类器时，如果设计得当，模型本身可能会输出一个概率分布（例如，如果你在开源模型中添加一个softmax层）。然后就像处理任何分类器的阈值问题一样对待它。

### 持续改进

评估不是一次性的。计划进行重新训练或提示优化：
-   在初次部署后，收集模型出错或不确定的案例。用这些案例来更新微调（将它们包含在下一个版本的训练集中）或用澄清说明来更新你的提示。
-   或许定期在新数据上进行评估，以确保性能保持稳定。如果出现了原始数据中没有的新的类别细微差别，你可能需要添加一些例子并微调一个更新版本，或者在系统提示中加入这一点。

通过结合自动化的置信度测量和定期的人工审计，你可以保持高准确性并稳步改进模型。鉴于用户强调准确性，这些步骤对于确保模型的分类可以被信任和验证至关重要。

## 5. 微调最佳实践和平台

如果你决定在4PT分类数据上微调一个LLM是值得的（对于像这样的自定义分类法，通常是值得的），这里有一些推荐的实践和平台选项：

### 微调策略

-   **选择正确的基础模型**：理想情况下，从一个已经擅长理解文本和遵循指令的模型开始。例如，OpenAI的`gpt-3.5-turbo`是使用他们平台时一个强大的基础模型——它内置了指令遵循能力。如果使用开源模型，Llama-2的指令微调变体（如Llama-2-Chat）或T5/Flan-T5模型可能很合适。由于任务是分类，如果LLM不是强制性的，你也可以考虑从一个面向分类的模型开始（例如，针对学术文本的DeBERTa或SciBERT）。但假设我们想要一个LLM，一个较小的生成模型可以被训练来输出类别名称。一些从业者以生成的方式微调GPT风格的模型（输入提示 -> 期望的补全作为标签）。另一种方法是在一个预训练模型上附加一个分类头（在HuggingFace Transformers中很常见）。例如，可以取Llama-2 transformer，并将最后一层替换为一个新的4类输出层，并以传统的分类方式进行训练。这通常能带来更快的收敛和直接的概率。缺点是你失去了一些生成式思维链可能提供的“可解释性”。然而，由于准确性至关重要，如果使用开源模型，分类头可能是一个极好的方法。
-   **准备高质量的训练数据**：这一点怎么强调都不为过。以一致的方式格式化你的数据——例如，你可以创建JSONL行，每行包含：`{ "prompt": "<论文文本或摘要>\n将其分类为[类型1, 类型2, 类型3, 类型4]:", "completion": "类型3" }`，这是OpenAI的微调格式；或者作为HuggingFace的输入-目标对（文本作为输入，标签作为目标）。确保输入的文本能代表模型将要看到的内容（如果你总是在摘要上微调，它可能无法泛化到全文）。如果全文对于基础模型的上下文来说太长，你可能需要截断或策略性地选择论文的部分进行训练。需要进行一些实验：比如尝试在摘要上训练 vs. 在全文内容上训练，看看哪个能带来更好的性能。此外，如果标注是由人类使用编码手册完成的，他们留下的任何笔记或理由都可能是有用的额外信号（尽管可能仅有标签就足够了）。
-   **使用验证集**：留出一部分数据在微调期间进行评估。这有助于避免过拟合。如果训练时间过长，微调有时会使一个大型模型在某些模式上变得过于自信但却是错误的。在验证性能上进行早停是很有用的。如果使用OpenAI的平台，你提供训练和验证文件，他们会报告准确率等指标。使用HuggingFace，你可以监控每个epoch的指标。
-   **学习率和轮次（epochs）**：对于大型模型，通常最好使用小的学习率和少的轮次（它们已经学习了语言；你只是在微调它们）。OpenAI的微调默认使用几个轮次。对于在Llama-2上进行LoRA微调，也许1-3个轮次，学习率在~1e-5到1e-4之间可以作为一个起点（这取决于数据集大小；更多的数据可以允许更多的轮次）。关键是不要“忘记”基础模型的能力——只是增强它们。如果完全微调风险太大，一些技术如逐步解冻（Gradual Unfreezing）或适配器模块（Adapter modules）可能会有帮助。
-   **保留指令微调特性**：如果你在一个聊天模型上用纯粹的“输入->标签”对进行微调，它有时会失去聊天风格。为了保留指令遵循能力，最好在微调数据中包含指令作为提示。例如，在微调数据中加入一个系统消息：“你是一个专家分类器……”然后是文章内容，补全部分就只是类别。这样模型就保留了它处于一个指令设置中的认知。OpenAI的文档通常建议在微调聊天模型时，在前面加上类似`“<|im_start|>system…<|im_end|><|im_start|>user…<|im_end|><|im_start|>assistant… label …<|im_end|>”`的格式。遵循模型的推荐格式很重要（OpenAI为微调GPT-3.5进行分类任务提供了指南[68]）。如果使用HuggingFace和一个指令微调的模型权重，尝试将输入格式化为指令。
-   **参数高效的方法**：如果自己运行，考虑使用LoRA（低秩自适应）或QLoRA，如果GPU内存是一个问题。LoRA可以在单个高端GPU上微调一个70B的模型，只训练小的秩矩阵，结果却出奇地好。已经证明，LoRA微调在许多任务中可以与完全微调相媲美，并且它们会产生一个与基础模型合并的小文件。这对于部署来说非常好，因为你可以保持基础模型不变，只为分类任务应用LoRA权重。像Hugging Face的PEFT库这样的平台使这变得很简单。

### 平台

-   **OpenAI微调API**：对于GPT-3.5类模型来说最简单。你上传你的JSONL文件，在OpenAI的服务器上运行微调作业，然后得到一个像`ft:gpt-3.5-turbo:your-org:custom-classifier:2025-08-15`这样的模型名称。然后你可以像调用ChatGPT一样用API调用这个模型。OpenAI处理扩展和部署。他们有文档和最佳实践指南（例如，他们建议即使在微调中也为分类使用有意义的提示，并对补全的长度等持谨慎态度）[68]。缺点是成本（你为训练令牌和使用令牌付费），以及一些限制（模型可能有最大输入长度，OpenAI有时不允许在某些较新的模型上进行微调，或者可能需要等待名单）。但如果部署没有限制且预算允许，这是一个获得定制GPT模型并可能获得出色性能的便捷途径。
-   **Hugging Face**：选项包括在本地使用`transformers`库或使用Hugging Face的云服务。在本地，你可以使用`Trainer` API在你的数据上微调一个像`facebook/llama-2-13b-chat-hf`这样的模型。Hugging Face提供了脚本以及`AutoTrain`平台（一个UI，你只需上传数据，它就会为你训练模型，包括文本分类模型）。还有`Hugging Face Hub Inference`端点，微调后，你可以在那里部署模型进行远程推理。如果使用较小的模型（如7B或13B），你甚至可以在单GPU机器上运行它进行推理。对于70B模型，你需要多个GPU或使用服务（Hugging Face有Jumping Labs，或者Amazon SageMaker可以托管大型模型）。这里的一个优势是庞大的社区——许多人已经为分类任务微调了Llama或Flan，所以有现成的经验可以遵循。
-   **LoRA微调工具**：例如，PEFT（参数高效微调库）和`lora.py`脚本，或用于大型模型训练的特定库如DeepSpeed或FTI。如果数据集适中，你通常可以在单个48GB的GPU上（甚至在24GB的GPU上使用QLoRA 4位量化）对7B或13B模型进行LoRA微调。对于70B模型，你可能需要多GPU或4位技术。网上有用于Llama 2 LoRA微调的Google Colab笔记本和Kaggle脚本——这可能是快速实验的一种方式。
-   **托管服务**：许多云提供商现在都提供微调即服务。例如，Azure OpenAI镜像了OpenAI的API（用于GPT模型）。Amazon Bedrock或Google的Vertex可能允许微调它们各自的模型（Google允许微调较小的T5模型；他们未来可能允许微调PaLM）。这些服务可以处理繁重的工作并提供一个可用的端点。权衡是成本和潜在的供应商锁定。
-   **试验合成数据**：如果标注数据有限，一个有趣的最佳实践是使用GPT-4本身来生成更多的训练示例。例如，你可以给GPT-4提供大纲或主题，并要求它写一个可以被分类为各种类型的段落，然后将这些作为额外数据来微调一个较小的模型。这基本上是从GPT-4到你的模型的知识蒸馏。Harriet的博客做了类似的事情，他们使用GPT-4来标注数据以微调GPT-3.5[19]。这可以显著提升较小模型的性能，达到他们声称的“GPT-4级别”。只需确保合成的示例是现实且多样的（并且可能与真实示例混合，这样模型就不会学习到AI生成文本的“套路”）。

最后，在部署前彻底测试微调过的模型。有时微调会引入奇怪的错误（也许它学会了总是输出某个短语，或者变得过于确定）。将它与你基于提示的GPT-4基线在一个基准集上进行比较。如果微调模型几乎同样好，你可能会出于成本原因选择它。如果不是，你可能会决定继续使用提示更大的模型，或者探索中等大小的模型（比如微调一个30B参数的模型可能是一个最佳平衡点）。

记录过程也很有帮助——跟踪你使用了哪些超参数和数据版本，这样你以后可以重现或改进微调。

**总结**：微调是为4PT分类实现高准确性和效率的强大方法。通过精心的准备和正确的平台（OpenAI为了简便，或HuggingFace/LoRA为了更多控制），你可以创建一个在其权重中编码了4PT编码手册的模型，从而产生快速且一致的分类。

## 结论与建议

基于此分析，推荐的路径如下：

1.  **从GPT-4 (32k) 或 Claude 开始**，以建立一个性能上限——用完整的论文文本和4PT指令来提示它，看看效果如何。这很可能会得到非常强的结果（考虑到GPT-4的著名能力，甚至可能达到人类水平的分类[3]）。如果该性能令人满意且预算允许，你可以在你的流程中直接通过API使用GPT-4或Claude（用LangChain或DSPy来管理提示和处理结果）。确保你集成了置信度检查（例如，通过`logprobs`或多次运行）来标记不确定性以供人工审查[59][60]。

2.  **如果需要降低成本或想要更多控制权，请在你的数据集上微调一个模型**。一种有效的方法是使用GPT-4（或你最好的基于提示的模型）来辅助——例如，让它标注更多的数据或提供理由——然后用这些来微调一个较小的模型，如GPT-3.5或Llama-2。在验证集上监控其准确性并进行迭代。有了一个调优良好的模型，你就可以通过OpenAI的端点或在你自己的基础设施上部署它，以进行大批量分类。

3.  **为了大规模自动化，利用像LangChain这样的框架来协调整个过程**——从加载文档，应用模型（无论是API还是本地模型），到存储输出[40]。LangChain简单的链式范式应该足够了，但如果你增加了像检索或多轮交互这样的步骤，像Haystack或LlamaIndex这样的框架可以提供帮助。

4.  **始终关注评估**——定期在标注集上计算准确率/F1分数，并注意性能漂移。使用模型的置信度信号来决定何时需要人工复核[69]。通过这样做，你将创建一个强大的、由LLM驱动的分类系统，它能准确地应用4PT框架，并能被研究人员信任用于分析学术文本。

---

**来源**:

- OpenAI & Anthropic 模型能力[3][7]、上下文长度[2][6]及任务性能。
- 从业者关于微调与提示的见解[29][31]及提示工程技巧。
- LangChain及流程工具使用示例[39][43]。
- 使用对数概率进行不确定性估计[59][60]。
- 比较模型（Gemini vs GPT-4 vs Claude）的实验结果[9]。
- Harriet关于廉价微调分类器的案例研究[19][31]。

[1] GPT-4 | OpenAI
https://openai.com/index/gpt-4-research/
[2] [5] It looks like GPT-4-32k is rolling out | Hacker News
https://news.ycombinator.com/item?id=35841460
[3] [7] Claude 2 vs GPT-4 reddit (2023) – Claude Ai
https://claudeai.wiki/claude-2-vs-gpt-4-reddit/
[4] [10] [19] [24] [31]  Fine-tune a better-than-GPT-4 classifier for $30 | Harriet
https://hrharriet.com/blog/fine-tune-a-better-than-gpt-4-classifier-for-30/
[6] Introducing 100K Context Windows - Anthropic
https://www.anthropic.com/news/100k-context-windows
[8] [9] [11] [34] [35] [56] Best Model for Text Classification: Gemini Pro, GPT-4 or Claude2?
https://www.vellum.ai/blog/best-at-text-classification-gemini-pro-gpt-4-or-claude2
[12] [13] [14] [17] How Does Llama-2 Compare to GPT-4/3.5 and Other AI Language Models
https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/
[15] [16] Democratizing AI: MosaicML's Impact on the Open-Source LLM ...
https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact
[18] mosaicml/mpt-7b - Hugging Face
https://huggingface.co/mosaicml/mpt-7b
[20] [21] [22] [23] Prompt Engineering or Fine-Tuning: An Empirical Assessment of LLMs for Code
https://arxiv.org/html/2310.10508v2
[25] [26] [29] [30] [32] [33] Fine-Tuning GPT-4: A Practical Guide | by why amit | Medium
https://medium.com/@whyamit101/fine-tuning-gpt-4-a-practical-guide-f98942126429
[27] Comparison of Prompt Engineering and Fine-Tuning Strategies in ...
https://pubmed.ncbi.nlm.nih.gov/38370673/
[28] On the limitations of large language models in clinical diagnosis
https://www.academia.edu/126627253/On_the_limitations_of_large_language_models_in_clinical_diagnosis
[36] [64] Self-Consistency | Prompt Engineering Guide
https://www.promptingguide.ai/techniques/consistency
[37] [38] [39] [40] [43] [44] Classification with Langchain — ChatOpenAI | by Sathya Priya A | Medium
https://medium.com/@spriya2809/classification-with-langchain-chatopenai-4d54e1eb88dd
[41] How to load PDFs - ️ LangChain
https://python.langchain.com/docs/how_to/document_loader_pdf/
[42] UnstructuredPDFLoader - ️ LangChain
https://python.langchain.com/docs/integrations/document_loaders/unstructured_pdfloader/
[45] Pipelines - Haystack Documentation
https://docs.haystack.deepset.ai/docs/pipelines
[46] Haystack — Part 2 of LLM series. Introducing Haystack Pipelines | by Tituslhy | MITB For All | Medium
https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multiverse-part-2-haystack-c6af2548df04
[47] [48] DocumentLanguageClassifier - Haystack Documentation - Deepset
https://docs.haystack.deepset.ai/v2.1/docs/documentlanguageclassifier
[49] The Tools Landscape for LLM Pipelines Orchestration (Part 1)
https://newsletter.theaiedge.io/p/the-tools-landscape-for-llm-pipelines
[50] Validating the RAG Performance of LangChain vs Haystack | Blog
https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-langchain-vs-haystack
[51] DSPy
https://dspy.ai/
[52] DSPy: The framework for programming—not prompting—language ...
https://github.com/stanfordnlp/dspy
[53] Pipelines & Prompt Optimization with DSPy - Drew Breunig
https://www.dbreunig.com/2024/12/12/pipelines-prompt-optimization-with-dspy.html
[54] Using DSPy for LLM and RAG: Declarative, Modular AI Pipelines ...
https://medium.com/@bravekjh/using-dspy-for-llm-and-rag-declarative-modular-ai-pipelines-with-python-61733b44ba48
[55] What is Haystack? An introduction to the NLP framework - Apify Blog
https://blog.apify.com/what-is-haystack-nlp-framework/
[57] [58] [59] [60] [61] [62] [63] [66] [69] Estimating LLM classification confidence with log probabilities (logprobs) – Eric Jinks
https://ericjinks.com/blog/2025/logprobs/
[65] Benchmarking LLMs via Uncertainty Quantification - OpenReview
https://openreview.net/forum?id=L0oSfTroNE&referrer=%5Bthe%20profile%20of%20Jianhui%20Pang%5D(%2Fprofile%3Fid%3D~Jianhui_Pang1)
[67] Peeking Behind the Curtain: How to Gauge GPT's Confidence
https://www.ai.moda/en/blog/gpt-confidence-log-probabilities
[68] OpenAI fine tuning for classification alternative - Stack Overflow
https://stackoverflow.com/questions/76142362/openai-fine-tuning-for-classification-alternative